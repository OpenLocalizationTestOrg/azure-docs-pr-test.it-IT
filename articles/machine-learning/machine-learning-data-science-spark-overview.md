---
title: Panoramica dell'analisi scientifica dei dati con Spark in Azure HDInsight | Microsoft Docs
description: "Il toolkit Spark MLlib introduce importanti funzionalità di modellazione di Machine Learning nell'ambiente distribuito HDInsight."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: a4e1de99-a554-4240-9647-2c6d669593c8
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/15/2017
ms.author: deguhath;bradsev;gokuma
ms.openlocfilehash: 379b32f4e533f48f1593a97e73737a0c5bfb9135
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 07/11/2017
---
# <a name="overview-of-data-science-using-spark-on-azure-hdinsight"></a><span data-ttu-id="84c2c-103">Panoramica dell'analisi scientifica dei dati con Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="84c2c-103">Overview of data science using Spark on Azure HDInsight</span></span>
[!INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

<span data-ttu-id="84c2c-104">Questa raccolta di argomenti illustra come usare HDInsight Spark per completare attività comuni di analisi scientifica dei dati, come l'inserimento di dati, la progettazione di funzionalità, la modellazione e la valutazione di modelli.</span><span class="sxs-lookup"><span data-stu-id="84c2c-104">This suite of topics shows how to use HDInsight Spark to complete common data science tasks such as data ingestion, feature engineering, modeling, and model evaluation.</span></span> <span data-ttu-id="84c2c-105">Come dati di esempio sono stati presi i dati relativi alle corse e alle tariffe dei taxi di New York nel 2013.</span><span class="sxs-lookup"><span data-stu-id="84c2c-105">The data used is a sample of the 2013 NYC taxi trip and fare dataset.</span></span> <span data-ttu-id="84c2c-106">I modelli creati includono la regressione logistica e quella lineare, foreste casuali e alberi con boosting a gradienti.</span><span class="sxs-lookup"><span data-stu-id="84c2c-106">The models built include logistic and linear regression, random forests, and gradient boosted trees.</span></span> <span data-ttu-id="84c2c-107">Gli argomenti illustrano anche come archiviare questi modelli nell'archiviazione BLOB di Azure (WASB) e come classificare e valutare le relative prestazioni predittive.</span><span class="sxs-lookup"><span data-stu-id="84c2c-107">The topics also show how to store these models in Azure blob storage (WASB) and how to score and evaluate their predictive performance.</span></span> <span data-ttu-id="84c2c-108">Argomenti più avanzati illustrano le modalità di training dei modelli con la convalida incrociata e lo sweep di iperparametri.</span><span class="sxs-lookup"><span data-stu-id="84c2c-108">More advanced topics cover how models can be trained using cross-validation and hyper-parameter sweeping.</span></span> <span data-ttu-id="84c2c-109">Questo argomento di panoramica fa riferimento agli argomenti che descrivono anche come impostare il cluster Spark necessario per completare i passaggi delle tre procedure dettagliate illustrate.</span><span class="sxs-lookup"><span data-stu-id="84c2c-109">This overview topic also references the topics that describe how to set up the Spark cluster that you need to complete the steps in the walkthroughs provided.</span></span> 

## <a name="spark-and-mllib"></a><span data-ttu-id="84c2c-110">Spark e MLlib</span><span class="sxs-lookup"><span data-stu-id="84c2c-110">Spark and MLlib</span></span>
<span data-ttu-id="84c2c-111">[Spark](http://spark.apache.org/) è un framework open source di elaborazione parallela che supporta l'elaborazione in memoria per migliorare le prestazioni delle applicazioni analitiche di Big Data.</span><span class="sxs-lookup"><span data-stu-id="84c2c-111">[Spark](http://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</span></span> <span data-ttu-id="84c2c-112">Il motore di elaborazione Spark è costruito per la velocità, la semplicità d'uso e le analisi sofisticate.</span><span class="sxs-lookup"><span data-stu-id="84c2c-112">The Spark processing engine is built for speed, ease of use, and sophisticated analytics.</span></span> <span data-ttu-id="84c2c-113">Le funzionalità di calcolo distribuite in memoria rendono Spark uno strumento valido per l'esecuzione di algoritmi iterativi utilizzati in calcoli grafici e di Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="84c2c-113">Spark's in-memory distributed computation capabilities make it a good choice for the iterative algorithms used in machine learning and graph computations.</span></span> <span data-ttu-id="84c2c-114">[MLlib](http://spark.apache.org/mllib/) è la libreria scalabile per il Machine Learning di Spark che introduce funzionalità di modellazione algoritmica nell'ambiente distribuito.</span><span class="sxs-lookup"><span data-stu-id="84c2c-114">[MLlib](http://spark.apache.org/mllib/) is Spark's scalable machine learning library that brings the algorithmic modeling capabilities to this distributed environment.</span></span> 

## <a name="hdinsight-spark"></a><span data-ttu-id="84c2c-115">HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="84c2c-115">HDInsight Spark</span></span>
<span data-ttu-id="84c2c-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) è la soluzione ospitata in Azure di Spark open source.</span><span class="sxs-lookup"><span data-stu-id="84c2c-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is the Azure hosted offering of open-source Spark.</span></span> <span data-ttu-id="84c2c-117">Include anche il supporto per **notebook di Jupyter PySpark** nel cluster Spark in grado di eseguire query Spark SQL interattive per trasformare, filtrare e visualizzare i dati archiviati in BLOB di Azure (WASB).</span><span class="sxs-lookup"><span data-stu-id="84c2c-117">It also includes support for **Jupyter PySpark notebooks** on the Spark cluster that can run Spark SQL interactive queries for transforming, filtering, and visualizing data stored in Azure Blobs (WASB).</span></span> <span data-ttu-id="84c2c-118">PySpark è l'API di Python per Spark.</span><span class="sxs-lookup"><span data-stu-id="84c2c-118">PySpark is the Python API for Spark.</span></span> <span data-ttu-id="84c2c-119">I frammenti di codice che consentono di creare le soluzioni e mostrano i tracciati rilevanti per la visualizzazione dei dati sono eseguiti qui in notebook di Jupyter installati in cluster Spark.</span><span class="sxs-lookup"><span data-stu-id="84c2c-119">The code snippets that provide the solutions and show the relevant plots to visualize the data here run in Jupyter notebooks installed on the Spark clusters.</span></span> <span data-ttu-id="84c2c-120">La procedura di modellazione riportata in questi argomenti include anche codice che illustra come eseguire il training, valutare, salvare e usare ogni tipo di modello.</span><span class="sxs-lookup"><span data-stu-id="84c2c-120">The modeling steps in these topics contain code that shows how to train, evaluate, save, and consume each type of model.</span></span> 

## <a name="setup-spark-clusters-and-jupyter-notebooks"></a><span data-ttu-id="84c2c-121">Configurazione: cluster Spark e notebook di Jupyter</span><span class="sxs-lookup"><span data-stu-id="84c2c-121">Setup: Spark clusters and Jupyter notebooks</span></span>
<span data-ttu-id="84c2c-122">La procedura di installazione e il codice forniti in questa procedura dettagliata sono per l'uso di un HDInsight Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="84c2c-122">Setup steps and code are provided in this walkthrough for using an HDInsight Spark 1.6.</span></span> <span data-ttu-id="84c2c-123">Ma vengono forniti i notebook di Jupyter per i cluster HDInsight sia Spark 1.6 sia Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="84c2c-123">But Jupyter notebooks are provided for both HDInsight Spark 1.6 and Spark 2.0 clusters.</span></span> <span data-ttu-id="84c2c-124">+Vengono inoltre forniti una descrizione dei notebook e i relativi collegamenti nel [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) per il repository GitHub che li contiene.</span><span class="sxs-lookup"><span data-stu-id="84c2c-124">A description of the notebooks and links to them are provided in the [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) for the GitHub repository containing them.</span></span> <span data-ttu-id="84c2c-125">Inoltre, il codice in questo esempio e nei notebook collegati è generico e funzionerà in qualsiasi cluster Spark.</span><span class="sxs-lookup"><span data-stu-id="84c2c-125">Moreover, the code here and in the linked notebooks is generic and should work on any Spark cluster.</span></span> <span data-ttu-id="84c2c-126">Se non si usa HDInsight Spark, i passaggi di configurazione e gestione del cluster possono essere leggermente diversi rispetto a quanto illustrato qui.</span><span class="sxs-lookup"><span data-stu-id="84c2c-126">If you are not using HDInsight Spark, the cluster setup and management steps may be slightly different from what is shown here.</span></span> <span data-ttu-id="84c2c-127">Per praticità, ecco i collegamenti per i notebook di Jupyter per Spark 1.6 (da eseguire nel kernek di pySpark del server Notebook di Jupyter) e Spark 2.0 (da eseguire nel kernel di pySpark3 del server Notebook di Jupyter):</span><span class="sxs-lookup"><span data-stu-id="84c2c-127">For convenience, here are the links to the Jupyter notebooks for Spark 1.6 (to be run in the pySpark kernel of the Jupyter Notebook server) and  Spark 2.0 (to be run in the pySpark3 kernel of the Jupyter Notebook server):</span></span>

### <a name="spark-16-notebooks"></a><span data-ttu-id="84c2c-128">Notebook Spark 1.6</span><span class="sxs-lookup"><span data-stu-id="84c2c-128">Spark 1.6 notebooks</span></span>
<span data-ttu-id="84c2c-129">Questi notebook devono essere eseguiti nel kernel di pySpark del server notebook di Jupyter.</span><span class="sxs-lookup"><span data-stu-id="84c2c-129">These notebooks are to be run in the pySpark kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="84c2c-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): fornisce informazioni su come eseguire l'esplorazione dei dati, la modellazione e l'assegnazione del punteggio con diversi algoritmi.</span><span class="sxs-lookup"><span data-stu-id="84c2c-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): Provides information on how to perform data exploration, modeling, and scoring with several different algorithms.</span></span>
- <span data-ttu-id="84c2c-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): include gli argomenti nel notebook numero 1 e modella lo sviluppo usando l'ottimizzazione degli iperparametri e la convalida incrociata.</span><span class="sxs-lookup"><span data-stu-id="84c2c-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): Includes topics in notebook #1, and model development using hyperparameter tuning and cross-validation.</span></span>
- <span data-ttu-id="84c2c-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): illustra come rendere operativo un modello salvato usando Python sui cluster di HDInsight.</span><span class="sxs-lookup"><span data-stu-id="84c2c-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): Shows how to operationalize a saved model using Python on HDInsight clusters.</span></span>

### <a name="spark-20-notebooks"></a><span data-ttu-id="84c2c-133">Notebook Spark 2.0</span><span class="sxs-lookup"><span data-stu-id="84c2c-133">Spark 2.0 notebooks</span></span>
<span data-ttu-id="84c2c-134">Questi notebook devono essere eseguiti nel kernel di pySpark3 del server notebook di Jupyter.</span><span class="sxs-lookup"><span data-stu-id="84c2c-134">These notebooks are to be run in the pySpark3 kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="84c2c-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): questo file fornisce informazioni su come eseguire l'esplorazione dei dati, la modellazione e l'assegnazione del punteggio nei cluster Spark 2.0 usando i dati relativi alle corse e alle tariffe dei taxi di NYC descritti [qui](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="84c2c-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): This file provides information on how to perform data exploration, modeling, and scoring in Spark 2.0 clusters using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span> <span data-ttu-id="84c2c-136">Questo notebook può essere un buon punto di partenza per esplorare velocemente il codice fornito per Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="84c2c-136">This notebook may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> <span data-ttu-id="84c2c-137">Per un notebook più dettagliato che analizza i dati dei taxi di NYC, vedere il notebook successivo di questo elenco.</span><span class="sxs-lookup"><span data-stu-id="84c2c-137">For a more detailed notebook analyzes the NYC Taxi data, see the next notebook in this list.</span></span> <span data-ttu-id="84c2c-138">Vedere le note dopo l'elenco che confrontano questi notebook.</span><span class="sxs-lookup"><span data-stu-id="84c2c-138">See the notes following this list that compare these notebooks.</span></span> 
- <span data-ttu-id="84c2c-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): questo file mostra come eseguire la gestione dei dati (Spark SQL e operazioni del frame di dati), l'esplorazione, il modellamento e l'assegnazione del punteggio utilizzando il set di dati delle corse e delle tariffe dei taxi di NYC descritti [qui](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span><span class="sxs-lookup"><span data-stu-id="84c2c-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span>
- <span data-ttu-id="84c2c-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): questo file mostra come eseguire la gestione dei dati (Spark SQL e operazioni del frame di dati), l'esplorazione, il modellamento e l'assegnazione del punteggio utilizzando il set di dati relativi alle partenze puntuali dei voli del 2011 e 2012 di una compagnia area.</span><span class="sxs-lookup"><span data-stu-id="84c2c-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the well-known Airline On-time departure dataset from 2011 and 2012.</span></span> <span data-ttu-id="84c2c-141">Abbiamo integrato il set di dati della compagnia aerea con i dati delle condizioni atmosferiche dell'aeroporto (ad esempio, velocità del vento, temperatura, altitudine e così via) prima del modellamento, in modo da poterli includere nel modello.</span><span class="sxs-lookup"><span data-stu-id="84c2c-141">We integrated the airline dataset with the airport weather data (e.g. windspeed, temperature, altitude etc.) prior to modeling, so these weather features can be included in the model.</span></span>

<!-- -->

> [!NOTE]
> <span data-ttu-id="84c2c-142">Il set di dati della compagnia aerea è stato aggiunto ai notebook Spark 2.0 per illustrare al meglio l'uso degli algoritmi di classificazione.</span><span class="sxs-lookup"><span data-stu-id="84c2c-142">The airline dataset was added to the Spark 2.0 notebooks to better illustrate the use of classification algorithms.</span></span> <span data-ttu-id="84c2c-143">Vedere i collegamenti seguenti per informazioni sul set di dati delle partenze puntuali dei voli della compagnia aerea e sul set di dati delle condizioni atmosferiche:</span><span class="sxs-lookup"><span data-stu-id="84c2c-143">See the following links for information about airline on-time departure dataset and weather dataset:</span></span>

>- <span data-ttu-id="84c2c-144">Dati sulle partenze puntuali dei voli della compagnia aerea: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span><span class="sxs-lookup"><span data-stu-id="84c2c-144">Airline on-time departure data: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span></span>

>- <span data-ttu-id="84c2c-145">Dati sulle condizioni atmosferiche dell'aeroporto: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span><span class="sxs-lookup"><span data-stu-id="84c2c-145">Airport weather data: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span></span> 
> 
> 

<!-- -->

<!-- -->

> [!NOTE]
<span data-ttu-id="84c2c-146">L'esecuzione dei notebook Spark 2.0 sui dati dei taxi di NYC e sui ritardi dei voli può impiegare 10 minuti o più (in base alla dimensione del cluster HDI).</span><span class="sxs-lookup"><span data-stu-id="84c2c-146">The Spark 2.0 notebooks on the NYC taxi and airline flight delay data-sets can take 10 mins or more to run (depending on the size of your HDI cluster).</span></span> <span data-ttu-id="84c2c-147">Il primo notebook dell'elenco precedente illustra molti aspetti dell'esplorazione dei dati, della visualizzazione e dell'addestramento del modello ML in un notebook che richiede meno tempo di esecuzione con dataset di NYC ricampionati, in cui i file dei dati di taxi e tariffe sono stati precedentemente uniti: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) Questo notebook termina in un tempo molto più breve (2-3 minuti) e può essere un buon punto di partenza per esplorare rapidamente il codice fornito per Spark 2.0.</span><span class="sxs-lookup"><span data-stu-id="84c2c-147">The first notebook in the above list shows many aspects of the data exploration, visualization and ML model training in a notebook that takes less time to run with down-sampled NYC data set, in which the taxi and fare files have been pre-joined: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) This notebook takes a much shorter time to finish (2-3 mins) and may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> 

<!-- -->

<span data-ttu-id="84c2c-148">Per indicazioni sull'operazionalizzazione di un modello Spark 2.0 e di un modello di consumo per l'assegnazione del punteggio, vedere il [documento Spark 1.6 sul consumo](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) per un esempio in cui viene mostrata la procedura.</span><span class="sxs-lookup"><span data-stu-id="84c2c-148">For guidance on the operationalization of a Spark 2.0 model and model consumption for scoring, see the [Spark 1.6 document on consumption](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) for an example outlining the steps required.</span></span> <span data-ttu-id="84c2c-149">Per utilizzare questa procedura su Spark 2.0, sostituire il file del codice Python con [questo file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span><span class="sxs-lookup"><span data-stu-id="84c2c-149">To use this on Spark 2.0, replace the Python code file with [this file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span></span>

### <a name="prerequisites"></a><span data-ttu-id="84c2c-150">Prerequisiti</span><span class="sxs-lookup"><span data-stu-id="84c2c-150">Prerequisites</span></span>
<span data-ttu-id="84c2c-151">Le procedure seguenti riguardano Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="84c2c-151">The following procedures are related to Spark 1.6.</span></span> <span data-ttu-id="84c2c-152">Per la versione Spark 2.0 usare i notebook le cui descrizioni e collegamenti sono riportati sopra.</span><span class="sxs-lookup"><span data-stu-id="84c2c-152">For  the Spark 2.0 version, use the notebooks described and linked to previously.</span></span> 

<span data-ttu-id="84c2c-153">1. È necessario avere una sottoscrizione di Azure.</span><span class="sxs-lookup"><span data-stu-id="84c2c-153">1.You must have an Azure subscription.</span></span> <span data-ttu-id="84c2c-154">Se non è già disponibile, vedere l'articolo che illustra [come ottenere una versione di valutazione gratuita di Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span><span class="sxs-lookup"><span data-stu-id="84c2c-154">If you do not already have one, see [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>

<span data-ttu-id="84c2c-155">2. Per completare questa procedura dettagliata è necessario un cluster HDInsight Spark 1.6.</span><span class="sxs-lookup"><span data-stu-id="84c2c-155">2.You need a Spark 1.6 cluster to complete this walkthrough.</span></span> <span data-ttu-id="84c2c-156">Per crearne uno, vedere le istruzioni fornite in [Introduzione: creare cluster Apache Spark in Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="84c2c-156">To create one, see the instructions provided in [Get started: create Apache Spark on Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span></span> <span data-ttu-id="84c2c-157">Il tipo e la versione del cluster vengono specificati tramite il menu **Selezionare il tipo di cluster** .</span><span class="sxs-lookup"><span data-stu-id="84c2c-157">The cluster type and version is specified from the **Select Cluster Type** menu.</span></span> 

![Configurare il cluster](./media/machine-learning-data-science-spark-overview/spark-cluster-on-portal.png)

<!-- -->

> [!NOTE]
> <span data-ttu-id="84c2c-159">Per un argomento che illustra come usare Scala anziché Python per completare le attività per un processo di analisi scientifica dei dati end-to-end, vedere l'articolo sull' [analisi scientifica dei dati tramite Scala con Spark in Azure](machine-learning-data-science-process-scala-walkthrough.md).</span><span class="sxs-lookup"><span data-stu-id="84c2c-159">For a topic that shows how to use Scala rather than Python to complete tasks for an end-to-end data science process, see the [Data Science using Scala with Spark on Azure](machine-learning-data-science-process-scala-walkthrough.md).</span></span>
> 
> 

<!-- -->

> [!INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]
> 
> 

## <a name="the-nyc-2013-taxi-data"></a><span data-ttu-id="84c2c-160">Dati dei taxi di NYC 2013</span><span class="sxs-lookup"><span data-stu-id="84c2c-160">The NYC 2013 Taxi data</span></span>
<span data-ttu-id="84c2c-161">I dati relativi alle corse dei taxi della città di New York sono costituiti da file con valori delimitati da virgole (CSV) compressi di dimensioni pari a circa 20 GB (circa 48 GB non compressi), che includono informazioni su oltre 173 milioni di corse singole e sulle tariffe pagate per ognuna.</span><span class="sxs-lookup"><span data-stu-id="84c2c-161">The NYC Taxi Trip data is about 20 GB of compressed comma-separated values (CSV) files (~48 GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</span></span> <span data-ttu-id="84c2c-162">Il record di ogni corsa include la località e l'orario di partenza e di arrivo, il numero di patente anonimo (del tassista) e il numero di licenza (ID univoco del taxi).</span><span class="sxs-lookup"><span data-stu-id="84c2c-162">Each trip record includes the pick up and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</span></span> <span data-ttu-id="84c2c-163">I dati sono relativi a tutte le corse per l'anno 2013 e vengono forniti nei due set di dati seguenti per ciascun mese:</span><span class="sxs-lookup"><span data-stu-id="84c2c-163">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</span></span>

1. <span data-ttu-id="84c2c-164">I file CSV "trip_data" contengono i dettagli delle corse, ad esempio il numero dei passeggeri, i punti di partenza e di arrivo, la durata e la lunghezza della corsa.</span><span class="sxs-lookup"><span data-stu-id="84c2c-164">The 'trip_data' CSV files contain trip details, such as number of passengers, pick up and dropoff points, trip duration, and trip length.</span></span> <span data-ttu-id="84c2c-165">Di seguito vengono forniti alcuni record di esempio:</span><span class="sxs-lookup"><span data-stu-id="84c2c-165">Here are a few sample records:</span></span>
   
        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868
2. <span data-ttu-id="84c2c-166">I file con estensione csv "trip_fare" contengono i dettagli della tariffa pagata per ciascuna corsa, ad esempio tipo di pagamento, importo, soprattassa e tasse, mance e pedaggi e l'importo totale pagato.</span><span class="sxs-lookup"><span data-stu-id="84c2c-166">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</span></span> <span data-ttu-id="84c2c-167">Di seguito vengono forniti alcuni record di esempio:</span><span class="sxs-lookup"><span data-stu-id="84c2c-167">Here are a few sample records:</span></span>
   
        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5

<span data-ttu-id="84c2c-168">È stato prelevato un campione pari allo 0,1% di questi file e i file CSV trip\_data e trip\_fare sono stati uniti in un singolo set di dati da usare come set di dati di input per questa procedura dettagliata.</span><span class="sxs-lookup"><span data-stu-id="84c2c-168">We have taken a 0.1% sample of these files and joined the trip\_data and trip\_fare CVS files into a single dataset to use as the input dataset for this walkthrough.</span></span> <span data-ttu-id="84c2c-169">La chiave univoca che consente di unire trip\_data e trip\_fare è composta dai campi: medallion, hack\_licence e pickup\_datetime.</span><span class="sxs-lookup"><span data-stu-id="84c2c-169">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</span></span> <span data-ttu-id="84c2c-170">Ogni record del set di dati contiene gli attributi seguenti che rappresentano una corsa dei taxi di NYC:</span><span class="sxs-lookup"><span data-stu-id="84c2c-170">Each record of the dataset contains the following attributes representing a NYC Taxi trip:</span></span>

| <span data-ttu-id="84c2c-171">Campo</span><span class="sxs-lookup"><span data-stu-id="84c2c-171">Field</span></span> | <span data-ttu-id="84c2c-172">Breve descrizione</span><span class="sxs-lookup"><span data-stu-id="84c2c-172">Brief Description</span></span> |
| --- | --- |
| <span data-ttu-id="84c2c-173">medallion</span><span class="sxs-lookup"><span data-stu-id="84c2c-173">medallion</span></span> |<span data-ttu-id="84c2c-174">Numero di licenza anonimo (ID univoco del taxi).</span><span class="sxs-lookup"><span data-stu-id="84c2c-174">Anonymized taxi medallion (unique taxi id)</span></span> |
| <span data-ttu-id="84c2c-175">hack_license</span><span class="sxs-lookup"><span data-stu-id="84c2c-175">hack_license</span></span> |<span data-ttu-id="84c2c-176">Numero di licenza anonimo di vetture a noleggio</span><span class="sxs-lookup"><span data-stu-id="84c2c-176">Anonymized Hackney Carriage License number</span></span> |
| <span data-ttu-id="84c2c-177">vendor_id</span><span class="sxs-lookup"><span data-stu-id="84c2c-177">vendor_id</span></span> |<span data-ttu-id="84c2c-178">ID del fornitore del taxi</span><span class="sxs-lookup"><span data-stu-id="84c2c-178">Taxi vendor id</span></span> |
| <span data-ttu-id="84c2c-179">rate_code</span><span class="sxs-lookup"><span data-stu-id="84c2c-179">rate_code</span></span> |<span data-ttu-id="84c2c-180">Tariffe dei taxi di NYC</span><span class="sxs-lookup"><span data-stu-id="84c2c-180">NYC taxi rate of fare</span></span> |
| <span data-ttu-id="84c2c-181">store_and_fwd_flag</span><span class="sxs-lookup"><span data-stu-id="84c2c-181">store_and_fwd_flag</span></span> |<span data-ttu-id="84c2c-182">Contrassegno di registrazione e inoltro</span><span class="sxs-lookup"><span data-stu-id="84c2c-182">Store and forward flag</span></span> |
| <span data-ttu-id="84c2c-183">pickup_datetime</span><span class="sxs-lookup"><span data-stu-id="84c2c-183">pickup_datetime</span></span> |<span data-ttu-id="84c2c-184">Data e ora di partenza</span><span class="sxs-lookup"><span data-stu-id="84c2c-184">Pick up date & time</span></span> |
| <span data-ttu-id="84c2c-185">dropoff_datetime</span><span class="sxs-lookup"><span data-stu-id="84c2c-185">dropoff_datetime</span></span> |<span data-ttu-id="84c2c-186">Data e ora di arrivo</span><span class="sxs-lookup"><span data-stu-id="84c2c-186">Dropoff date & time</span></span> |
| <span data-ttu-id="84c2c-187">pickup_hour</span><span class="sxs-lookup"><span data-stu-id="84c2c-187">pickup_hour</span></span> |<span data-ttu-id="84c2c-188">Ora di partenza</span><span class="sxs-lookup"><span data-stu-id="84c2c-188">Pick up hour</span></span> |
| <span data-ttu-id="84c2c-189">pickup_week</span><span class="sxs-lookup"><span data-stu-id="84c2c-189">pickup_week</span></span> |<span data-ttu-id="84c2c-190">Settimana dell'anno di partenza</span><span class="sxs-lookup"><span data-stu-id="84c2c-190">Pick up week of the year</span></span> |
| <span data-ttu-id="84c2c-191">weekday</span><span class="sxs-lookup"><span data-stu-id="84c2c-191">weekday</span></span> |<span data-ttu-id="84c2c-192">Giorno della settimana (intervallo 1-7)</span><span class="sxs-lookup"><span data-stu-id="84c2c-192">Weekday (range 1-7)</span></span> |
| <span data-ttu-id="84c2c-193">passenger_count</span><span class="sxs-lookup"><span data-stu-id="84c2c-193">passenger_count</span></span> |<span data-ttu-id="84c2c-194">Numero di passeggeri in una corsa del taxi</span><span class="sxs-lookup"><span data-stu-id="84c2c-194">Number of passengers in a taxi trip</span></span> |
| <span data-ttu-id="84c2c-195">trip_time_in_secs</span><span class="sxs-lookup"><span data-stu-id="84c2c-195">trip_time_in_secs</span></span> |<span data-ttu-id="84c2c-196">Tempo delle corse in secondi</span><span class="sxs-lookup"><span data-stu-id="84c2c-196">Trip time in seconds</span></span> |
| <span data-ttu-id="84c2c-197">trip_distance</span><span class="sxs-lookup"><span data-stu-id="84c2c-197">trip_distance</span></span> |<span data-ttu-id="84c2c-198">Distanza delle corse percorsa in miglia</span><span class="sxs-lookup"><span data-stu-id="84c2c-198">Trip distance traveled in miles</span></span> |
| <span data-ttu-id="84c2c-199">pickup_longitude</span><span class="sxs-lookup"><span data-stu-id="84c2c-199">pickup_longitude</span></span> |<span data-ttu-id="84c2c-200">Longitudine di partenza</span><span class="sxs-lookup"><span data-stu-id="84c2c-200">Pick up longitude</span></span> |
| <span data-ttu-id="84c2c-201">pickup_latitude</span><span class="sxs-lookup"><span data-stu-id="84c2c-201">pickup_latitude</span></span> |<span data-ttu-id="84c2c-202">Latitudine di partenza</span><span class="sxs-lookup"><span data-stu-id="84c2c-202">Pick up latitude</span></span> |
| <span data-ttu-id="84c2c-203">dropoff_longitude</span><span class="sxs-lookup"><span data-stu-id="84c2c-203">dropoff_longitude</span></span> |<span data-ttu-id="84c2c-204">Longitudine di arrivo</span><span class="sxs-lookup"><span data-stu-id="84c2c-204">Dropoff longitude</span></span> |
| <span data-ttu-id="84c2c-205">dropoff_latitude</span><span class="sxs-lookup"><span data-stu-id="84c2c-205">dropoff_latitude</span></span> |<span data-ttu-id="84c2c-206">Latitudine di arrivo</span><span class="sxs-lookup"><span data-stu-id="84c2c-206">Dropoff latitude</span></span> |
| <span data-ttu-id="84c2c-207">direct_distance</span><span class="sxs-lookup"><span data-stu-id="84c2c-207">direct_distance</span></span> |<span data-ttu-id="84c2c-208">Distanza diretta tra le località di partenza e di arrivo</span><span class="sxs-lookup"><span data-stu-id="84c2c-208">Direct distance between pick up and dropoff locations</span></span> |
| <span data-ttu-id="84c2c-209">payment_type</span><span class="sxs-lookup"><span data-stu-id="84c2c-209">payment_type</span></span> |<span data-ttu-id="84c2c-210">Tipo di pagamento (contanti, carta di credito e così via).</span><span class="sxs-lookup"><span data-stu-id="84c2c-210">Payment type (cas, credit-card etc.)</span></span> |
| <span data-ttu-id="84c2c-211">fare_amount</span><span class="sxs-lookup"><span data-stu-id="84c2c-211">fare_amount</span></span> |<span data-ttu-id="84c2c-212">Imposto della tariffa in</span><span class="sxs-lookup"><span data-stu-id="84c2c-212">Fare amount in</span></span> |
| <span data-ttu-id="84c2c-213">surcharge</span><span class="sxs-lookup"><span data-stu-id="84c2c-213">surcharge</span></span> |<span data-ttu-id="84c2c-214">Sovrapprezzo</span><span class="sxs-lookup"><span data-stu-id="84c2c-214">Surcharge</span></span> |
| <span data-ttu-id="84c2c-215">mta_tax</span><span class="sxs-lookup"><span data-stu-id="84c2c-215">mta_tax</span></span> |<span data-ttu-id="84c2c-216">Tassa MTA</span><span class="sxs-lookup"><span data-stu-id="84c2c-216">Mta tax</span></span> |
| <span data-ttu-id="84c2c-217">tip_amount</span><span class="sxs-lookup"><span data-stu-id="84c2c-217">tip_amount</span></span> |<span data-ttu-id="84c2c-218">Importo delle mance</span><span class="sxs-lookup"><span data-stu-id="84c2c-218">Tip amount</span></span> |
| <span data-ttu-id="84c2c-219">tolls_amount</span><span class="sxs-lookup"><span data-stu-id="84c2c-219">tolls_amount</span></span> |<span data-ttu-id="84c2c-220">Importo dei pedaggi</span><span class="sxs-lookup"><span data-stu-id="84c2c-220">Tolls amount</span></span> |
| <span data-ttu-id="84c2c-221">total_amount</span><span class="sxs-lookup"><span data-stu-id="84c2c-221">total_amount</span></span> |<span data-ttu-id="84c2c-222">Importo totale</span><span class="sxs-lookup"><span data-stu-id="84c2c-222">Total amount</span></span> |
| <span data-ttu-id="84c2c-223">tipped</span><span class="sxs-lookup"><span data-stu-id="84c2c-223">tipped</span></span> |<span data-ttu-id="84c2c-224">Mancia lasciata (0/1 per no o sì)</span><span class="sxs-lookup"><span data-stu-id="84c2c-224">Tipped (0/1 for no or yes)</span></span> |
| <span data-ttu-id="84c2c-225">tip_class</span><span class="sxs-lookup"><span data-stu-id="84c2c-225">tip_class</span></span> |<span data-ttu-id="84c2c-226">Categoria mance (0: $ 0, 1: $ 0-5, 2: $ 6-10, 3: $ 11-20, 4: > $ 20)</span><span class="sxs-lookup"><span data-stu-id="84c2c-226">Tip class (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span></span> |

## <a name="execute-code-from-a-jupyter-notebook-on-the-spark-cluster"></a><span data-ttu-id="84c2c-227">Eseguire il codice da Jupyter Notebook nel cluster Spark</span><span class="sxs-lookup"><span data-stu-id="84c2c-227">Execute code from a Jupyter notebook on the Spark cluster</span></span>
<span data-ttu-id="84c2c-228">È possibile avviare il notebook di Jupyter dal portale di Azure.</span><span class="sxs-lookup"><span data-stu-id="84c2c-228">You can launch the Jupyter Notebook from the Azure portal.</span></span> <span data-ttu-id="84c2c-229">Trovare il cluster Spark nel dashboard e fare clic su di esso per aprire la relativa pagina di gestione.</span><span class="sxs-lookup"><span data-stu-id="84c2c-229">Find your Spark cluster on your dashboard and click it to enter management page for your cluster.</span></span> <span data-ttu-id="84c2c-230">Per aprire il notebook associato al cluster Spark, fare clic su **Dashboard cluster** -> **Notebook di Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="84c2c-230">To open the notebook associated with the Spark cluster, click **Cluster Dashboards** -> **Jupyter Notebook** .</span></span>

![Dashboard del cluster](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

<span data-ttu-id="84c2c-232">Per accedere ai notebook di Jupyter, è anche possibile passare a ***https://NOMECLUSTER.azurehdinsight.net/jupyter***.</span><span class="sxs-lookup"><span data-stu-id="84c2c-232">You can also browse to ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** to access the Jupyter Notebooks.</span></span> <span data-ttu-id="84c2c-233">Sostituire NOMECLUSTER nell'URL con il nome del proprio cluster.</span><span class="sxs-lookup"><span data-stu-id="84c2c-233">Replace the CLUSTERNAME part of this URL with the name of your own cluster.</span></span> <span data-ttu-id="84c2c-234">Per accedere ai notebook, sarà necessaria la password dell'account amministratore.</span><span class="sxs-lookup"><span data-stu-id="84c2c-234">You need the password for your admin account to access the notebooks.</span></span>

![Sfogliare i notebook di Jupyter](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

<span data-ttu-id="84c2c-236">Selezionare PySpark per visualizzare una directory con alcuni esempi di notebook predefiniti che usano l'API PySpark. I notebook contenenti gli esempi di codice per questa serie di argomenti su Spark sono disponibili in [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark).</span><span class="sxs-lookup"><span data-stu-id="84c2c-236">Select PySpark to see a directory that contains a few examples of pre-packaged notebooks that use the PySpark API.The notebooks that contain the code samples for this suite of Spark topic are available at [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span></span>

<span data-ttu-id="84c2c-237">È possibile caricare i notebook direttamente da [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) nel server del notebook di Jupyter sul cluster Spark.</span><span class="sxs-lookup"><span data-stu-id="84c2c-237">You can upload the notebooks directly from [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) to the Jupyter notebook server on your Spark cluster.</span></span> <span data-ttu-id="84c2c-238">Nella home page di Jupyter fare clic sul pulsante **Upload** (Carica) a destra nella schermata.</span><span class="sxs-lookup"><span data-stu-id="84c2c-238">On the home page of your Jupyter, click the **Upload** button on the right part of the screen.</span></span> <span data-ttu-id="84c2c-239">Verrà visualizzata una finestra di esplorazione file,</span><span class="sxs-lookup"><span data-stu-id="84c2c-239">It opens a file explorer.</span></span> <span data-ttu-id="84c2c-240">in cui è possibile incollare l'URL di GitHub (contenuto non elaborato) del notebook e fare clic su **Open** (Apri).</span><span class="sxs-lookup"><span data-stu-id="84c2c-240">Here you can paste the GitHub (raw content) URL of the Notebook and click **Open**.</span></span> 

<span data-ttu-id="84c2c-241">Il nome del file verrà visualizzato nell'elenco di file Jupyter e sarà di nuovo disponibile il pulsante **Upload** (Carica).</span><span class="sxs-lookup"><span data-stu-id="84c2c-241">You see the file name on your Jupyter file list with an **Upload** button again.</span></span> <span data-ttu-id="84c2c-242">Fare clic sul pulsante **Upload** (Carica).</span><span class="sxs-lookup"><span data-stu-id="84c2c-242">Click this **Upload** button.</span></span> <span data-ttu-id="84c2c-243">A questo punto, il notebook è stato importato.</span><span class="sxs-lookup"><span data-stu-id="84c2c-243">Now you have imported the notebook.</span></span> <span data-ttu-id="84c2c-244">Ripetere i passaggi per caricare gli altri notebook di questa procedura dettagliata.</span><span class="sxs-lookup"><span data-stu-id="84c2c-244">Repeat these steps to upload the other notebooks from this walkthrough.</span></span>

> [!TIP]
> <span data-ttu-id="84c2c-245">È possibile fare clic con il pulsante destro del mouse sui collegamenti nel browser e scegliere **Copia collegamento** per ottenere l'URL del contenuto non elaborato di GitHub.</span><span class="sxs-lookup"><span data-stu-id="84c2c-245">You can right-click the links on your browser and select **Copy Link** to get the github raw content URL.</span></span> <span data-ttu-id="84c2c-246">È quindi possibile incollare questo URL nella finestra di dialogo per il caricamento di file in Jupyter.</span><span class="sxs-lookup"><span data-stu-id="84c2c-246">You can paste this URL into the Jupyter Upload file explorer dialog box.</span></span>
> 
> 

<span data-ttu-id="84c2c-247">A questo punto è possibile:</span><span class="sxs-lookup"><span data-stu-id="84c2c-247">Now you can:</span></span>

* <span data-ttu-id="84c2c-248">Fare clic sul notebook per visualizzare il codice.</span><span class="sxs-lookup"><span data-stu-id="84c2c-248">See the code by clicking the notebook.</span></span>
* <span data-ttu-id="84c2c-249">Eseguire ogni cella premendo **MAIUSC+INVIO**.</span><span class="sxs-lookup"><span data-stu-id="84c2c-249">Execute each cell by pressing **SHIFT-ENTER**.</span></span>
* <span data-ttu-id="84c2c-250">Eseguire l'intero notebook facendo clic su **Cell** -> **Run** (Cella -> Esegui).</span><span class="sxs-lookup"><span data-stu-id="84c2c-250">Run the entire notebook by clicking on **Cell** -> **Run**.</span></span>
* <span data-ttu-id="84c2c-251">Usare la visualizzazione automatica delle query.</span><span class="sxs-lookup"><span data-stu-id="84c2c-251">Use the automatic visualization of queries.</span></span>

> [!TIP]
> <span data-ttu-id="84c2c-252">Il kernel Pyspark visualizza automaticamente l'output delle query SQL (HiveQL).</span><span class="sxs-lookup"><span data-stu-id="84c2c-252">The PySpark kernel automatically visualizes the output of SQL (HiveQL) queries.</span></span> <span data-ttu-id="84c2c-253">È possibile scegliere tra diversi tipi di visualizzazioni (tabella, a torta, a linee, ad area o a barre) usando i pulsanti del menu **Type** (Tipo) nel notebook:</span><span class="sxs-lookup"><span data-stu-id="84c2c-253">You are given the option to select among several different types of visualizations (Table, Pie, Line, Area, or Bar) by using the **Type** menu buttons in the notebook:</span></span>
> 
> 

![Curva ROC di regressione logistica per approccio generico](./media/machine-learning-data-science-spark-overview/pyspark-jupyter-autovisualization.png)

## <a name="whats-next"></a><span data-ttu-id="84c2c-255">Passaggi successivi</span><span class="sxs-lookup"><span data-stu-id="84c2c-255">What's next?</span></span>
<span data-ttu-id="84c2c-256">Dopo aver configurato un cluster HDInsight Spark e avere caricato i notebook di Jupyter, è possibile seguire gli argomenti corrispondenti a questi tre notebook di PySpark.</span><span class="sxs-lookup"><span data-stu-id="84c2c-256">Now that you are set up with an HDInsight Spark cluster and have uploaded the Jupyter notebooks, you are ready to work through the topics that correspond to the three PySpark notebooks.</span></span> <span data-ttu-id="84c2c-257">Tali argomenti illustrano come esplorare i dati e quindi come creare e utilizzare i modelli.</span><span class="sxs-lookup"><span data-stu-id="84c2c-257">They show how to explore your data and then how to create and consume models.</span></span> <span data-ttu-id="84c2c-258">Il notebook relativo a modellazione ed esplorazione avanzate dei dati mostra come includere la convalida incrociata, lo sweep di iperparametri e la valutazione del modello.</span><span class="sxs-lookup"><span data-stu-id="84c2c-258">The advanced data exploration and modeling notebook shows how to include cross-validation, hyper-parameter sweeping, and model evaluation.</span></span> 

<span data-ttu-id="84c2c-259">**Esplorazione e modellazione avanzate dei dati con Spark:** esplorare il set di dati ed eseguire operazioni di creazione, assegnazione dei punteggi e valutazione di modelli di Machine Learning seguendo quanto illustrato nell'argomento [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) (Creare modelli di classificazione binaria e regressione per i dati con il toolkit Spark MLlib).</span><span class="sxs-lookup"><span data-stu-id="84c2c-259">**Data Exploration and modeling with Spark:** Explore the dataset and create, score, and evaluate the machine learning models by working through the [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) topic.</span></span>

<span data-ttu-id="84c2c-260">**Uso dei modelli:** per informazioni su come valutare i modelli di regressione e di classificazione creati in questo argomento, vedere [Assegnare punteggi a modelli di apprendimento automatico compilati con Spark](machine-learning-data-science-spark-model-consumption.md).</span><span class="sxs-lookup"><span data-stu-id="84c2c-260">**Model consumption:** To learn how to score the classification and regression models created in this topic, see [Score and evaluate Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md).</span></span>

<span data-ttu-id="84c2c-261">**Convalida incrociata e sweep di iperparametri**: vedere [Esplorazione e modellazione avanzate dei dati con Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) per informazioni su come istruire i modelli sulla convalida incrociata e lo sweep di iperparametri</span><span class="sxs-lookup"><span data-stu-id="84c2c-261">**Cross-validation and hyperparameter sweeping**: See [Advanced data exploration and modeling with Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) on how models can be trained using cross-validation and hyper-parameter sweeping</span></span>

