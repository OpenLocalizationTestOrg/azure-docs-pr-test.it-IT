---
title: Kernel per il notebook di Jupyter nei cluster Spark in Azure HDInsight | Microsoft Docs
description: Informazioni sui kernel PySpark, PySpark3 e Spark per i notebook di Jupyter disponibili con i cluster Spark in Azure HDInsight.
keywords: notebook jupyter in spark, spark jupyter
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="fd96d-104">Kernel per il notebook di Jupyter nei cluster Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="fd96d-105">I cluster HDInsight Spark forniscono kernel che è possibile usare con il notebook di Jupyter in Spark per testare le applicazioni.</span><span class="sxs-lookup"><span data-stu-id="fd96d-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="fd96d-106">Un kernel è un programma che esegue e interpreta il codice.</span><span class="sxs-lookup"><span data-stu-id="fd96d-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="fd96d-107">I tre kernel sono:</span><span class="sxs-lookup"><span data-stu-id="fd96d-107">The three kernels are:</span></span>

- <span data-ttu-id="fd96d-108">**PySpark** per le applicazioni scritte in Python2</span><span class="sxs-lookup"><span data-stu-id="fd96d-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="fd96d-109">**PySpark3** per le applicazioni scritte in Python3</span><span class="sxs-lookup"><span data-stu-id="fd96d-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="fd96d-110">**Spark** per le applicazioni scritte in Scala</span><span class="sxs-lookup"><span data-stu-id="fd96d-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="fd96d-111">In questo articolo viene illustrato come usare questi kernel e i vantaggi che ne derivano.</span><span class="sxs-lookup"><span data-stu-id="fd96d-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="fd96d-112">Prerequisiti</span><span class="sxs-lookup"><span data-stu-id="fd96d-112">Prerequisites</span></span>

* <span data-ttu-id="fd96d-113">Un cluster Apache Spark in HDInsight.</span><span class="sxs-lookup"><span data-stu-id="fd96d-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="fd96d-114">Per istruzioni, vedere l'articolo relativo alla [creazione di cluster Apache Spark in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="fd96d-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="fd96d-115">Creare un notebook di Jupyter in HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="fd96d-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="fd96d-116">Dal [Portale di Azure](https://portal.azure.com/), aprire il cluster.</span><span class="sxs-lookup"><span data-stu-id="fd96d-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="fd96d-117">Per le istruzioni vedere la sezione su come [elencare e visualizzare i cluster](hdinsight-administer-use-portal-linux.md#list-and-show-clusters).</span><span class="sxs-lookup"><span data-stu-id="fd96d-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="fd96d-118">Il cluster viene aperto in un nuovo pannello del portale.</span><span class="sxs-lookup"><span data-stu-id="fd96d-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="fd96d-119">Nella sezione **Collegamenti rapidi** fare clic su **Dashboard cluster** per aprire il pannello **Dashboard cluster**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="fd96d-120">Se non viene visualizzato **Collegamenti rapidi**, fare clic su **Panoramica** dal menu a sinistra del pannello.</span><span class="sxs-lookup"><span data-stu-id="fd96d-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="fd96d-121">![Notebook di Jupyter in Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Notebook di Jupyter in Spark")</span><span class="sxs-lookup"><span data-stu-id="fd96d-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="fd96d-122">Fare clic su **Notebook di Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="fd96d-123">Se richiesto, immettere le credenziali per il cluster.</span><span class="sxs-lookup"><span data-stu-id="fd96d-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="fd96d-124">È anche possibile accedere al notebook di Jupyter nel cluster Spark aprendo l'URL seguente nel browser.</span><span class="sxs-lookup"><span data-stu-id="fd96d-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="fd96d-125">Sostituire **CLUSTERNAME** con il nome del cluster:</span><span class="sxs-lookup"><span data-stu-id="fd96d-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="fd96d-126">Fare clic su **Nuovo** e quindi su **Pyspark**, **PySpark3** o **Spark** per creare un notebook.</span><span class="sxs-lookup"><span data-stu-id="fd96d-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="fd96d-127">Usare il kernel Spark per applicazioni Scala, il kernel PySpark per applicazioni Python2 e il kernel PySpark3 per applicazioni Python3.</span><span class="sxs-lookup"><span data-stu-id="fd96d-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="fd96d-128">![Kernel per il notebook di Jupyter in Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernel per il notebook di Jupyter in Spark")</span><span class="sxs-lookup"><span data-stu-id="fd96d-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="fd96d-129">Verrà aperto un notebook con il kernel selezionato.</span><span class="sxs-lookup"><span data-stu-id="fd96d-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="fd96d-130">Vantaggi offerti dall'uso dei kernel</span><span class="sxs-lookup"><span data-stu-id="fd96d-130">Benefits of using the kernels</span></span>

<span data-ttu-id="fd96d-131">Ecco alcuni vantaggi associati all'uso dei nuovi kernel con il notebook di Jupyter nei cluster HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="fd96d-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="fd96d-132">**Contesti predefiniti**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-132">**Preset contexts**.</span></span> <span data-ttu-id="fd96d-133">Con i kernel **PySpark**, **PySpark3** e **Spark** non è necessario impostare i contesti Spark o Hive in modo esplicito prima di iniziare a usare le applicazioni.</span><span class="sxs-lookup"><span data-stu-id="fd96d-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="fd96d-134">I contesti sono disponibili per impostazione predefinita.</span><span class="sxs-lookup"><span data-stu-id="fd96d-134">These are available by default.</span></span> <span data-ttu-id="fd96d-135">Questi contesti sono:</span><span class="sxs-lookup"><span data-stu-id="fd96d-135">These contexts are:</span></span>
   
   * <span data-ttu-id="fd96d-136">**sc** : per il contesto Spark</span><span class="sxs-lookup"><span data-stu-id="fd96d-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="fd96d-137">**sqlContext** : per il contesto Hive</span><span class="sxs-lookup"><span data-stu-id="fd96d-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="fd96d-138">Quindi non è necessario eseguire istruzioni simili alle seguenti per impostare i contesti:</span><span class="sxs-lookup"><span data-stu-id="fd96d-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="fd96d-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="fd96d-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="fd96d-140">È possibile invece usare direttamente i contesti preimpostati nell'applicazione.</span><span class="sxs-lookup"><span data-stu-id="fd96d-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="fd96d-141">**Celle magic**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-141">**Cell magics**.</span></span> <span data-ttu-id="fd96d-142">Il kernel PySpark offre alcuni "magic" predefiniti. Si tratta di comandi speciali che è possibile chiamare con `%%`, ad esempio `%%MAGIC` <args>.</span><span class="sxs-lookup"><span data-stu-id="fd96d-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="fd96d-143">Il comando magic deve essere la prima parola di una cella di codice e consente di generare più righe di contenuto.</span><span class="sxs-lookup"><span data-stu-id="fd96d-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="fd96d-144">La parola magic deve essere la prima della cella.</span><span class="sxs-lookup"><span data-stu-id="fd96d-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="fd96d-145">L'aggiunta di qualsiasi elemento prima del comando magic, anche un commento, causa un errore.</span><span class="sxs-lookup"><span data-stu-id="fd96d-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="fd96d-146">Per altre informazioni sui magic, vedere [questa pagina](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="fd96d-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="fd96d-147">La tabella seguente elenca i diversi magic disponibili tramite i kernel.</span><span class="sxs-lookup"><span data-stu-id="fd96d-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="fd96d-148">Magic</span><span class="sxs-lookup"><span data-stu-id="fd96d-148">Magic</span></span> | <span data-ttu-id="fd96d-149">Esempio</span><span class="sxs-lookup"><span data-stu-id="fd96d-149">Example</span></span> | <span data-ttu-id="fd96d-150">Descrizione</span><span class="sxs-lookup"><span data-stu-id="fd96d-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="fd96d-151">help</span><span class="sxs-lookup"><span data-stu-id="fd96d-151">help</span></span> |`%%help` |<span data-ttu-id="fd96d-152">Genera una tabella di tutti i magic disponibili con esempi e descrizioni</span><span class="sxs-lookup"><span data-stu-id="fd96d-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="fd96d-153">info</span><span class="sxs-lookup"><span data-stu-id="fd96d-153">info</span></span> |`%%info` |<span data-ttu-id="fd96d-154">Visualizza informazioni sulla sessione per l'endpoint Livy corrente</span><span class="sxs-lookup"><span data-stu-id="fd96d-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="fd96d-155">CONFIGURA</span><span class="sxs-lookup"><span data-stu-id="fd96d-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="fd96d-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="fd96d-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="fd96d-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="fd96d-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="fd96d-158">Configura i parametri per la creazione di una sessione.</span><span class="sxs-lookup"><span data-stu-id="fd96d-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="fd96d-159">Il flag force (-f) è obbligatorio se è già stata creata una sessione, affinché tale sessione venga eliminata e ricreata.</span><span class="sxs-lookup"><span data-stu-id="fd96d-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="fd96d-160">Visitare la pagina relativa al [corpo della richiesta POST/sessions di Livy](https://github.com/cloudera/livy#request-body) per un elenco dei parametri validi.</span><span class="sxs-lookup"><span data-stu-id="fd96d-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="fd96d-161">I parametri devono essere passati come una stringa JSON e devono essere nella riga successiva al magic, come illustrato nella colonna di esempio.</span><span class="sxs-lookup"><span data-stu-id="fd96d-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="fd96d-162">sql</span><span class="sxs-lookup"><span data-stu-id="fd96d-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="fd96d-163">Esegue una query Hive su sqlContext.</span><span class="sxs-lookup"><span data-stu-id="fd96d-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="fd96d-164">Se viene passato il parametro `-o` , il risultato della query viene salvato in modo permanente nel contesto Python %%local come frame di dati [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="fd96d-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="fd96d-165">local</span><span class="sxs-lookup"><span data-stu-id="fd96d-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="fd96d-166">Tutto il codice presente nelle righe successive viene eseguito localmente.</span><span class="sxs-lookup"><span data-stu-id="fd96d-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="fd96d-167">Deve trattarsi di codice Python2 valido indipendentemente dal kernel usato.</span><span class="sxs-lookup"><span data-stu-id="fd96d-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="fd96d-168">Anche se durante la creazione del notebook si è selezionato il kernel **PySpark3** o **Spark**, se si usa il magic `%%local` in una cella, tale cella deve contenere solo codice Python2 valido.</span><span class="sxs-lookup"><span data-stu-id="fd96d-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="fd96d-169">logs</span><span class="sxs-lookup"><span data-stu-id="fd96d-169">logs</span></span> |`%%logs` |<span data-ttu-id="fd96d-170">Visualizza i log per la sessione Livy corrente.</span><span class="sxs-lookup"><span data-stu-id="fd96d-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="fd96d-171">delete</span><span class="sxs-lookup"><span data-stu-id="fd96d-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="fd96d-172">Elimina una sessione specifica dell'endpoint Livy corrente.</span><span class="sxs-lookup"><span data-stu-id="fd96d-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="fd96d-173">Si noti che non è possibile eliminare la sessione avviata dal kernel stesso.</span><span class="sxs-lookup"><span data-stu-id="fd96d-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="fd96d-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="fd96d-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="fd96d-175">Elimina tutte le sessioni per l'endpoint Livy corrente, inclusa quella del notebook.</span><span class="sxs-lookup"><span data-stu-id="fd96d-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="fd96d-176">Il flag di forzatura -f è obbligatorio.</span><span class="sxs-lookup"><span data-stu-id="fd96d-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="fd96d-177">Oltre ai magic aggiunti dal kernel PySpark, è possibile anche usare i [magic IPython incorporati](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), tra cui `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="fd96d-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="fd96d-178">È possibile usare il magic `%%sh` per eseguire script e il blocco del codice nel nodo head del cluster.</span><span class="sxs-lookup"><span data-stu-id="fd96d-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="fd96d-179">**Visualizzazione automatica**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-179">**Auto visualization**.</span></span> <span data-ttu-id="fd96d-180">Il kernel **Pyspark** visualizza automaticamente l'output delle query Hive e SQL.</span><span class="sxs-lookup"><span data-stu-id="fd96d-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="fd96d-181">È possibile scegliere tra diversi tipi di visualizzazione, inclusi Table, Pie, Line, Area e Bar.</span><span class="sxs-lookup"><span data-stu-id="fd96d-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="fd96d-182">Parametri supportati con il magic %%sql</span><span class="sxs-lookup"><span data-stu-id="fd96d-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="fd96d-183">Il magic `%%sql` supporta parametri diversi che è possibile usare per controllare la tipologia di output che si riceve quando si eseguono query.</span><span class="sxs-lookup"><span data-stu-id="fd96d-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="fd96d-184">La tabella seguente elenca l'output.</span><span class="sxs-lookup"><span data-stu-id="fd96d-184">The following table lists the output.</span></span>

| <span data-ttu-id="fd96d-185">Parametro</span><span class="sxs-lookup"><span data-stu-id="fd96d-185">Parameter</span></span> | <span data-ttu-id="fd96d-186">Esempio</span><span class="sxs-lookup"><span data-stu-id="fd96d-186">Example</span></span> | <span data-ttu-id="fd96d-187">Descrizione</span><span class="sxs-lookup"><span data-stu-id="fd96d-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="fd96d-188">-o</span><span class="sxs-lookup"><span data-stu-id="fd96d-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="fd96d-189">Usare questo parametro per salvare in modo permanente il risultato della query nel contesto Python %%local come frame di dati [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="fd96d-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="fd96d-190">Il nome della variabile del frame di dati è il nome della variabile specificato.</span><span class="sxs-lookup"><span data-stu-id="fd96d-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="fd96d-191">-q</span><span class="sxs-lookup"><span data-stu-id="fd96d-191">-q</span></span> |`-q` |<span data-ttu-id="fd96d-192">Consente di disattivare le visualizzazioni per la cella.</span><span class="sxs-lookup"><span data-stu-id="fd96d-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="fd96d-193">Se non si vuole visualizzare automaticamente il contenuto di una cella ma solo acquisirlo come un frame di dati, usare `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="fd96d-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="fd96d-194">Se si vogliono disattivare le visualizzazioni senza acquisire i risultati, ad esempio per l'esecuzione di una query SQL come un'istruzione `CREATE TABLE`, usare `-q` senza specificare un argomento `-o`.</span><span class="sxs-lookup"><span data-stu-id="fd96d-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="fd96d-195">-m</span><span class="sxs-lookup"><span data-stu-id="fd96d-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="fd96d-196">Dove **METHOD** è **take** o **sample**. L'impostazione predefinita è **take**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="fd96d-197">Se il metodo è **take**, il kernel sceglie gli elementi dall'inizio del set di dati dei risultati specificato da MAXROWS, descritto più avanti in questa tabella.</span><span class="sxs-lookup"><span data-stu-id="fd96d-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="fd96d-198">Se il metodo è **sample**, il kernel campiona in modo casuale gli elementi del set di dati in base al parametro `-r` descritto di seguito in questa tabella.</span><span class="sxs-lookup"><span data-stu-id="fd96d-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="fd96d-199">-r</span><span class="sxs-lookup"><span data-stu-id="fd96d-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="fd96d-200">Qui **FRACTION** è un numero a virgola mobile compreso tra 0,0 e 1,0.</span><span class="sxs-lookup"><span data-stu-id="fd96d-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="fd96d-201">Se il metodo di campionamento per la query SQL è `sample`, il kernel campiona automaticamente in modo casuale la frazione specificata degli elementi del set di risultati.</span><span class="sxs-lookup"><span data-stu-id="fd96d-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="fd96d-202">Se si esegue una query SQL con gli argomenti `-m sample -r 0.01`, ad esempio, viene campionato in modo casuale l'1% delle righe dei risultati.</span><span class="sxs-lookup"><span data-stu-id="fd96d-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="fd96d-203">**MAXROWS** è un valore intero.</span><span class="sxs-lookup"><span data-stu-id="fd96d-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="fd96d-204">Il kernel limita il numero di righe di output a **MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="fd96d-205">Se **MAXROWS** è un numero negativo, ad esempio **-1**, il numero di righe nel set di risultati non è limitato.</span><span class="sxs-lookup"><span data-stu-id="fd96d-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="fd96d-206">**Esempio:**</span><span class="sxs-lookup"><span data-stu-id="fd96d-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="fd96d-207">L'istruzione precedente esegue le operazioni seguenti:</span><span class="sxs-lookup"><span data-stu-id="fd96d-207">The statement above does the following:</span></span>

* <span data-ttu-id="fd96d-208">Seleziona tutti i record da **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="fd96d-209">Dal momento che viene usato -q, disattiva la visualizzazione automatica.</span><span class="sxs-lookup"><span data-stu-id="fd96d-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="fd96d-210">Dal momento che si usa `-m sample -r 0.1 -n 500` , campiona in modo casuale il 10% delle righe di hivesampletable e limita la dimensione del set di risultati a 500 righe.</span><span class="sxs-lookup"><span data-stu-id="fd96d-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="fd96d-211">Infine, poiché è stato usato `-o query2` , salva anche l'oputput in un frame di dati denominato **query2**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="fd96d-212">Considerazioni per l'uso dei nuovi kernel</span><span class="sxs-lookup"><span data-stu-id="fd96d-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="fd96d-213">Indipendentemente dal kernel usato, se si lasciano i notebook in esecuzione vengono utilizzate risorse del cluster.</span><span class="sxs-lookup"><span data-stu-id="fd96d-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="fd96d-214">Con questi kernel, dato che i contesti sono preimpostati, uscendo semplicemente dal notebook non viene terminato il contesto e quindi le risorse del cluster restano in uso.</span><span class="sxs-lookup"><span data-stu-id="fd96d-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="fd96d-215">Quando non è più necessario usare il notebook, è consigliabile scegliere l'opzione **Close and Halt** (Chiudi e interrompi) dal menu **File** del notebook, in modo da terminare il contesto e uscire quindi dal notebook.</span><span class="sxs-lookup"><span data-stu-id="fd96d-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="fd96d-216">Di seguito sono riportati alcuni esempi</span><span class="sxs-lookup"><span data-stu-id="fd96d-216">Show me some examples</span></span>

<span data-ttu-id="fd96d-217">Quando si apre un Jupyter Notebook, vengono visualizzate due cartelle a livello di radice.</span><span class="sxs-lookup"><span data-stu-id="fd96d-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="fd96d-218">La cartella **PySpark** contiene notebook di esempio che usano il nuovo kernel **Python**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="fd96d-219">La cartella **Scala** contiene notebook di esempio che usano il kernel predefinito **Spark**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="fd96d-220">Per conoscere i diversi magic disponibili, è possibile aprire il notebook **00 - [READ ME FIRST] Spark Magic Kernel Features** dalla cartella **PySpark** o **Spark**.</span><span class="sxs-lookup"><span data-stu-id="fd96d-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="fd96d-221">Per informazioni su come realizzare diversi scenari usando i notebook Jupyter con cluster HDInsight Spark, è anche possibile usare gli altri notebook di esempio disponibili nelle due cartelle.</span><span class="sxs-lookup"><span data-stu-id="fd96d-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="fd96d-222">Dove sono archiviati i notebook</span><span class="sxs-lookup"><span data-stu-id="fd96d-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="fd96d-223">I notebook Jupyter vengono salvati nell'account di archiviazione associato al cluster nella cartella **/HdiNotebooks** .</span><span class="sxs-lookup"><span data-stu-id="fd96d-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="fd96d-224">I notebook, i file di testo e le cartelle che si creano da Jupyter sono accessibili dall'account di archiviazione.</span><span class="sxs-lookup"><span data-stu-id="fd96d-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="fd96d-225">Se si usa Jupyter per creare una cartella **myfolder** e un notebook **myfolder/mynotebook.ipynb**, ad esempio, è possibile accedere al notebook in `/HdiNotebooks/myfolder/mynotebook.ipynb` all'interno dell'account di archiviazione.</span><span class="sxs-lookup"><span data-stu-id="fd96d-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="fd96d-226">Analogamente, se si carica un notebook direttamente nell'account di archiviazione in `/HdiNotebooks/mynotebook1.ipynb`, il notebook è visibile anche da Jupyter.</span><span class="sxs-lookup"><span data-stu-id="fd96d-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="fd96d-227">I notebook vengono conservati nell'account di archiviazione anche dopo l'eliminazione del cluster.</span><span class="sxs-lookup"><span data-stu-id="fd96d-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="fd96d-228">La modalità di salvataggio dei notebook nell'account di archiviazione è compatibile con HDFS.</span><span class="sxs-lookup"><span data-stu-id="fd96d-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="fd96d-229">Se si usa SSH nel cluster, è quindi possibile usare comandi di gestione dei file come illustrato nel frammento seguente:</span><span class="sxs-lookup"><span data-stu-id="fd96d-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="fd96d-230">In caso di problemi di accesso all'account di archiviazione per il cluster, anche i notebook vengono salvati nel nodo head `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="fd96d-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="fd96d-231">Browser supportati</span><span class="sxs-lookup"><span data-stu-id="fd96d-231">Supported browser</span></span>

<span data-ttu-id="fd96d-232">I notebook di Jupyter nei cluster HDInsight Spark sono supportati solo su Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="fd96d-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="fd96d-233">Commenti e suggerimenti</span><span class="sxs-lookup"><span data-stu-id="fd96d-233">Feedback</span></span>
<span data-ttu-id="fd96d-234">I nuovi kernel sono ancora in una fase iniziale e si evolveranno nel tempo.</span><span class="sxs-lookup"><span data-stu-id="fd96d-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="fd96d-235">Questo potrebbe comportare un cambiamento delle API con l'evoluzione dei kernel.</span><span class="sxs-lookup"><span data-stu-id="fd96d-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="fd96d-236">Sono graditi commenti e suggerimenti in merito all'uso di questi nuovi kernel.</span><span class="sxs-lookup"><span data-stu-id="fd96d-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="fd96d-237">Saranno molto utili per la progettazione della versione finale di questi kernel.</span><span class="sxs-lookup"><span data-stu-id="fd96d-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="fd96d-238">È possibile inserire commenti o suggerimenti nella sezione **Commenti** in fondo a questo articolo.</span><span class="sxs-lookup"><span data-stu-id="fd96d-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="fd96d-239"><a name="seealso"></a>Vedere anche</span><span class="sxs-lookup"><span data-stu-id="fd96d-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="fd96d-240">Panoramica: Apache Spark su Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="fd96d-241">Scenari</span><span class="sxs-lookup"><span data-stu-id="fd96d-241">Scenarios</span></span>
* [<span data-ttu-id="fd96d-242">Spark con Business Intelligence: eseguire l'analisi interattiva dei dati con strumenti di Business Intelligence mediante Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="fd96d-243">Spark con Machine Learning: utilizzare Spark in HDInsight per l'analisi della temperatura di compilazione utilizzando dati HVAC</span><span class="sxs-lookup"><span data-stu-id="fd96d-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="fd96d-244">Spark con Machine Learning: usare Spark in HDInsight per prevedere i risultati del controllo degli alimenti</span><span class="sxs-lookup"><span data-stu-id="fd96d-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="fd96d-245">Streaming Spark: usare Spark in HDInsight per la creazione di applicazioni di streaming in tempo reale</span><span class="sxs-lookup"><span data-stu-id="fd96d-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="fd96d-246">Analisi dei log del sito Web mediante Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="fd96d-247">Creare ed eseguire applicazioni</span><span class="sxs-lookup"><span data-stu-id="fd96d-247">Create and run applications</span></span>
* [<span data-ttu-id="fd96d-248">Creare un'applicazione autonoma con Scala</span><span class="sxs-lookup"><span data-stu-id="fd96d-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="fd96d-249">Eseguire processi in modalità remota in un cluster Spark usando Livy</span><span class="sxs-lookup"><span data-stu-id="fd96d-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="fd96d-250">Strumenti ed estensioni</span><span class="sxs-lookup"><span data-stu-id="fd96d-250">Tools and extensions</span></span>
* [<span data-ttu-id="fd96d-251">Usare il plug-in degli strumenti HDInsight per IntelliJ IDEA per creare e inviare applicazioni Spark Scala</span><span class="sxs-lookup"><span data-stu-id="fd96d-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="fd96d-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely (Usare il plug-in Strumenti HDInsight per IntelliJ IDEA per eseguire il debug di applicazioni Spark in remoto)</span><span class="sxs-lookup"><span data-stu-id="fd96d-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="fd96d-253">Usare i notebook di Zeppelin con un cluster Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="fd96d-254">Usare pacchetti esterni con i notebook Jupyter</span><span class="sxs-lookup"><span data-stu-id="fd96d-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="fd96d-255">Installare Jupyter Notebook nel computer e connetterlo a un cluster HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="fd96d-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="fd96d-256">Gestire risorse</span><span class="sxs-lookup"><span data-stu-id="fd96d-256">Manage resources</span></span>
* [<span data-ttu-id="fd96d-257">Gestire le risorse del cluster Apache Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="fd96d-258">Tenere traccia ed eseguire il debug di processi in esecuzione nel cluster Apache Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="fd96d-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
