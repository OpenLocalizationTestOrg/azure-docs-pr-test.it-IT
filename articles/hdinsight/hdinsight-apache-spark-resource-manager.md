---
title: Gestire le risorse del cluster Apache Spark in Azure HDInsight | Documentazione Microsoft
description: Informazioni su come gestire le risorse per i cluster Spark in Azure HDInsight per ottenere prestazioni migliori.
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 9da7d4e3-458e-4296-a628-77b14643f7e4
ms.service: hdinsight
ms.custom: hdinsightactive
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/21/2017
ms.author: nitinme
ms.openlocfilehash: 952fa15162a40bccb3f8c7a88508556757ca6675
ms.sourcegitcommit: 02e69c4a9d17645633357fe3d46677c2ff22c85a
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 08/03/2017
---
# <a name="manage-resources-for-apache-spark-cluster-on-azure-hdinsight"></a><span data-ttu-id="299a9-103">Gestire le risorse del cluster Apache Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="299a9-103">Manage resources for Apache Spark cluster on Azure HDInsight</span></span> 

<span data-ttu-id="299a9-104">Questo articolo descrive come accedere a interfacce associate al cluster Spark, come l'interfaccia utente di Ambari, l'interfaccia utente di YARN e Server cronologia Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-104">In this article you will learn how to access the interfaces like Ambari UI, YARN UI, and the Spark History Server associated with your Spark cluster.</span></span> <span data-ttu-id="299a9-105">Si apprenderà anche come ottimizzare la configurazione del cluster per ottenere prestazioni ottimali.</span><span class="sxs-lookup"><span data-stu-id="299a9-105">You will also learn about how to tune the cluster configuration for optimal performance.</span></span>

<span data-ttu-id="299a9-106">**Prerequisiti:**</span><span class="sxs-lookup"><span data-stu-id="299a9-106">**Prerequisites:**</span></span>

<span data-ttu-id="299a9-107">È necessario disporre di quanto segue:</span><span class="sxs-lookup"><span data-stu-id="299a9-107">You must have the following:</span></span>

* <span data-ttu-id="299a9-108">Una sottoscrizione di Azure.</span><span class="sxs-lookup"><span data-stu-id="299a9-108">An Azure subscription.</span></span> <span data-ttu-id="299a9-109">Vedere [Ottenere una versione di valutazione gratuita di Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span><span class="sxs-lookup"><span data-stu-id="299a9-109">See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>
* <span data-ttu-id="299a9-110">Un cluster Apache Spark in HDInsight.</span><span class="sxs-lookup"><span data-stu-id="299a9-110">An Apache Spark cluster on HDInsight.</span></span> <span data-ttu-id="299a9-111">Per istruzioni, vedere l'articolo relativo alla [creazione di cluster Apache Spark in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="299a9-111">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="how-do-i-launch-the-ambari-web-ui"></a><span data-ttu-id="299a9-112">Come è possibile avviare l'interfaccia utente Web di Ambari?</span><span class="sxs-lookup"><span data-stu-id="299a9-112">How do I launch the Ambari Web UI?</span></span>
1. <span data-ttu-id="299a9-113">Dalla Schermata iniziale del [portale di Azure](https://portal.azure.com/)fare clic sul riquadro del cluster Spark (se è stato aggiunto sulla Schermata iniziale).</span><span class="sxs-lookup"><span data-stu-id="299a9-113">From the [Azure Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).</span></span> <span data-ttu-id="299a9-114">È anche possibile passare al cluster da **Esplora tutto** > **Cluster HDInsight**.</span><span class="sxs-lookup"><span data-stu-id="299a9-114">You can also navigate to your cluster under **Browse All** > **HDInsight Clusters**.</span></span>
2. <span data-ttu-id="299a9-115">Dal pannello del cluster Spark fare clic su **Dashboard**.</span><span class="sxs-lookup"><span data-stu-id="299a9-115">From the Spark cluster blade, click **Dashboard**.</span></span> <span data-ttu-id="299a9-116">Quando richiesto, immettere le credenziali di amministratore per il cluster di Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-116">When prompted, enter the admin credentials for the Spark cluster.</span></span>

    <span data-ttu-id="299a9-117">![Avviare Ambari](./media/hdinsight-apache-spark-resource-manager/hdinsight-launch-cluster-dashboard.png "Avviare Resource Manager")</span><span class="sxs-lookup"><span data-stu-id="299a9-117">![Launch Ambari](./media/hdinsight-apache-spark-resource-manager/hdinsight-launch-cluster-dashboard.png "Start Resource Manager")</span></span>
3. <span data-ttu-id="299a9-118">Questa operazione consente di avviare l'interfaccia utente Web di Ambari, come illustrato di seguito.</span><span class="sxs-lookup"><span data-stu-id="299a9-118">This should launch the Ambari Web UI, as shown below.</span></span>

    <span data-ttu-id="299a9-119">![Interfaccia utente Web Ambari](./media/hdinsight-apache-spark-resource-manager/ambari-web-ui.png "Interfaccia utente Web Ambari")</span><span class="sxs-lookup"><span data-stu-id="299a9-119">![Ambari Web UI](./media/hdinsight-apache-spark-resource-manager/ambari-web-ui.png "Ambari Web UI")</span></span>   

## <a name="how-do-i-launch-the-spark-history-server"></a><span data-ttu-id="299a9-120">Come è possibile avviare il Server cronologia Spark?</span><span class="sxs-lookup"><span data-stu-id="299a9-120">How do I launch the Spark History Server?</span></span>
1. <span data-ttu-id="299a9-121">Dalla Schermata iniziale del [portale di Azure](https://portal.azure.com/)fare clic sul riquadro del cluster Spark (se è stato aggiunto sulla Schermata iniziale).</span><span class="sxs-lookup"><span data-stu-id="299a9-121">From the [Azure Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).</span></span>
2. <span data-ttu-id="299a9-122">Dal pannello del cluster in **Collegamenti rapidi** fare clic su **Dashboard cluster**.</span><span class="sxs-lookup"><span data-stu-id="299a9-122">From the cluster blade, under **Quick Links**, click **Cluster Dashboard**.</span></span> <span data-ttu-id="299a9-123">Nel pannello **Dashboard cluster** fare clic su **Server cronologia Spark**.</span><span class="sxs-lookup"><span data-stu-id="299a9-123">In the **Cluster Dashboard** blade, click **Spark History Server**.</span></span>

    <span data-ttu-id="299a9-124">![Server cronologia Spark](./media/hdinsight-apache-spark-resource-manager/launch-history-server.png "Server cronologia Spark")</span><span class="sxs-lookup"><span data-stu-id="299a9-124">![Spark History Server](./media/hdinsight-apache-spark-resource-manager/launch-history-server.png "Spark History Server")</span></span>

    <span data-ttu-id="299a9-125">Quando richiesto, immettere le credenziali di amministratore per il cluster di Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-125">When prompted, enter the admin credentials for the Spark cluster.</span></span>

## <a name="how-do-i-launch-the-yarn-ui"></a><span data-ttu-id="299a9-126">Come è possibile avviare l'interfaccia utente di Yarn?</span><span class="sxs-lookup"><span data-stu-id="299a9-126">How do I launch the Yarn UI?</span></span>
<span data-ttu-id="299a9-127">È possibile usare l'interfaccia utente di YARN per il monitoraggio delle applicazioni attualmente in esecuzione nel cluster Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-127">You can use the YARN UI to monitor applications that are currently running on the Spark cluster.</span></span>

1. <span data-ttu-id="299a9-128">Nel pannello del cluster fare clic su **Dashboard cluster** e quindi su **YARN**.</span><span class="sxs-lookup"><span data-stu-id="299a9-128">From the cluster blade, click **Cluster Dashboard**, and then click **YARN**.</span></span>

    ![Avviare l'interfaccia utente di YARN](./media/hdinsight-apache-spark-resource-manager/launch-yarn-ui.png)

   > [!TIP]
   > <span data-ttu-id="299a9-130">In alternativa, è anche possibile avviare l'interfaccia utente di YARN dall'interfaccia utente di Ambari.</span><span class="sxs-lookup"><span data-stu-id="299a9-130">Alternatively, you can also launch the YARN UI from the Ambari UI.</span></span> <span data-ttu-id="299a9-131">Per avviare l'interfaccia utente di Ambari, nel pannello del cluster fare clic su **Dashboard cluster** e quindi su **Dashboard cluster HDInsight**.</span><span class="sxs-lookup"><span data-stu-id="299a9-131">To launch the Ambari UI, from the cluster blade, click **Cluster Dashboard**, and then click **HDInsight Cluster Dashboard**.</span></span> <span data-ttu-id="299a9-132">Nell'interfaccia utente di Ambari fare clic su **YARN**, su **Collegamenti rapidi**, sulla funzionalità di gestione risorse attiva e quindi fare clic su **ResourceManager UI**.</span><span class="sxs-lookup"><span data-stu-id="299a9-132">From the Ambari UI, click **YARN**, click **Quick Links**, click the active resource manager, and then click **ResourceManager UI**.</span></span>
   >
   >

## <a name="what-is-the-optimum-cluster-configuration-to-run-spark-applications"></a><span data-ttu-id="299a9-133">Qual è la configurazione cluster ottimale per eseguire le applicazioni Spark?</span><span class="sxs-lookup"><span data-stu-id="299a9-133">What is the optimum cluster configuration to run Spark applications?</span></span>
<span data-ttu-id="299a9-134">I tre parametri principali che possono essere usati per la configurazione di Spark, a seconda dei requisiti dell'applicazione, sono `spark.executor.instances`, `spark.executor.cores` e `spark.executor.memory`.</span><span class="sxs-lookup"><span data-stu-id="299a9-134">The three key parameters that can be used for Spark configuration depending on application requirements are `spark.executor.instances`, `spark.executor.cores`, and `spark.executor.memory`.</span></span> <span data-ttu-id="299a9-135">Un Executor è un processo avviato per un'applicazione Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-135">An Executor is a process launched for a Spark application.</span></span> <span data-ttu-id="299a9-136">Viene eseguito sul nodo di lavoro e svolge le attività per l'applicazione.</span><span class="sxs-lookup"><span data-stu-id="299a9-136">It runs on the worker node and is responsible to carry out the tasks for the application.</span></span> <span data-ttu-id="299a9-137">Il numero predefinito di executor e le relative dimensioni per ogni cluster vengono calcolati in base al numero di nodi di lavoro e alle relative dimensioni.</span><span class="sxs-lookup"><span data-stu-id="299a9-137">The default number of executors and the executor sizes for each cluster is calculated based on the number of worker nodes and the worker node size.</span></span> <span data-ttu-id="299a9-138">Questi dati vengono archiviati in `spark-defaults.conf` nei nodi head del cluster.</span><span class="sxs-lookup"><span data-stu-id="299a9-138">These are stored in `spark-defaults.conf` on the cluster head nodes.</span></span>

<span data-ttu-id="299a9-139">I tre parametri di configurazione possono essere configurati a livello di cluster, per tutte le applicazioni in esecuzione nel cluster, o possono anche essere specificati per ogni singola applicazione.</span><span class="sxs-lookup"><span data-stu-id="299a9-139">The three configuration parameters can be configured at the cluster level (for all applications that run on the cluster) or can be specified for each individual application as well.</span></span>

### <a name="change-the-parameters-using-ambari-ui"></a><span data-ttu-id="299a9-140">Modificare i parametri con l'interfaccia utente di Ambari</span><span class="sxs-lookup"><span data-stu-id="299a9-140">Change the parameters using Ambari UI</span></span>
1. <span data-ttu-id="299a9-141">Nell'interfaccia utente di Ambari fare clic su **Spark**, **Configs** (Configurazioni) e quindi espandere **Custom spark-defaults**.</span><span class="sxs-lookup"><span data-stu-id="299a9-141">From the Ambari UI click **Spark**, click **Configs**, and then expand **Custom spark-defaults**.</span></span>

    ![Impostare parametri con Ambari](./media/hdinsight-apache-spark-resource-manager/set-parameters-using-ambari.png)
2. <span data-ttu-id="299a9-143">I valori predefiniti sono appropriati per 4 applicazioni Spark in esecuzione contemporaneamente nel cluster.</span><span class="sxs-lookup"><span data-stu-id="299a9-143">The default values are good to have 4 Spark applications run concurrently on the cluster.</span></span> <span data-ttu-id="299a9-144">È possibile modificare questi valori nell'interfaccia utente, come illustrato di seguito.</span><span class="sxs-lookup"><span data-stu-id="299a9-144">You can changes these values from the user interface, as shown below.</span></span>

    ![Impostare parametri con Ambari](./media/hdinsight-apache-spark-resource-manager/set-executor-parameters.png)
3. <span data-ttu-id="299a9-146">Per salvare la configurazione, fare clic su **Save** .</span><span class="sxs-lookup"><span data-stu-id="299a9-146">Click **Save** to save the configuration changes.</span></span> <span data-ttu-id="299a9-147">Nella parte superiore della pagina verrà richiesto di riavviare tutti i servizi interessati.</span><span class="sxs-lookup"><span data-stu-id="299a9-147">At the top of the page, you will be prompted to restart all the affected services.</span></span> <span data-ttu-id="299a9-148">Fare clic su **Restart**.</span><span class="sxs-lookup"><span data-stu-id="299a9-148">Click **Restart**.</span></span>

    ![Riavviare i servizi](./media/hdinsight-apache-spark-resource-manager/restart-services.png)

### <a name="change-the-parameters-for-an-application-running-in-jupyter-notebook"></a><span data-ttu-id="299a9-150">Modificare i parametri per un'applicazione in esecuzione in Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="299a9-150">Change the parameters for an application running in Jupyter notebook</span></span>
<span data-ttu-id="299a9-151">Per le applicazioni in esecuzione in Jupyter Notebook è possibile usare `%%configure` magic per apportare le modifiche di configurazione.</span><span class="sxs-lookup"><span data-stu-id="299a9-151">For applications running in the Jupyter notebook, you can use the `%%configure` magic to make the configuration changes.</span></span> <span data-ttu-id="299a9-152">Idealmente, è necessario apportare le modifiche all'inizio dell'applicazione, prima di eseguire la prima cella di codice.</span><span class="sxs-lookup"><span data-stu-id="299a9-152">Ideally, you must make such changes at the beginning of the application, before you run your first code cell.</span></span> <span data-ttu-id="299a9-153">Ciò garantisce che la configurazione venga applicata alla sessione Livy quando viene creata.</span><span class="sxs-lookup"><span data-stu-id="299a9-153">This ensures that the configuration is applied to the Livy session, when it gets created.</span></span> <span data-ttu-id="299a9-154">Se si vuole modificare la configurazione in una fase successiva nell'applicazione, è necessario usare il parametro `-f` .</span><span class="sxs-lookup"><span data-stu-id="299a9-154">If you want to change the configuration at a later stage in the application, you must use the `-f` parameter.</span></span> <span data-ttu-id="299a9-155">Tuttavia, in questo modo tutte le operazioni eseguite nell'applicazione andranno perse.</span><span class="sxs-lookup"><span data-stu-id="299a9-155">However, by doing so all progress in the application will be lost.</span></span>

<span data-ttu-id="299a9-156">Il frammento di codice riportato di seguito mostra come modificare la configurazione per un'applicazione in esecuzione in Jupyter.</span><span class="sxs-lookup"><span data-stu-id="299a9-156">The snippet below shows how to change the configuration for an application running in Jupyter.</span></span>

    %%configure
    {"executorMemory": "3072M", "executorCores": 4, "numExecutors":10}

<span data-ttu-id="299a9-157">I parametri di configurazione devono essere passati come una stringa JSON e devono trovarsi nella riga successiva a magic, come illustrato nella colonna di esempio.</span><span class="sxs-lookup"><span data-stu-id="299a9-157">Configuration parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span>

### <a name="change-the-parameters-for-an-application-submitted-using-spark-submit"></a><span data-ttu-id="299a9-158">Modificare i parametri per un'applicazione inviata tramite spark-submit</span><span class="sxs-lookup"><span data-stu-id="299a9-158">Change the parameters for an application submitted using spark-submit</span></span>
<span data-ttu-id="299a9-159">Il comando seguente è un esempio di come modificare i parametri di configurazione per un'applicazione batch inviata tramite `spark-submit`.</span><span class="sxs-lookup"><span data-stu-id="299a9-159">Following command is an example of how to change the configuration parameters for a batch application that is submitted using `spark-submit`.</span></span>

    spark-submit --class <the application class to execute> --executor-memory 3072M --executor-cores 4 –-num-executors 10 <location of application jar file> <application parameters>

### <a name="change-the-parameters-for-an-application-submitted-using-curl"></a><span data-ttu-id="299a9-160">Modificare i parametri per un'applicazione inviata tramite cURL</span><span class="sxs-lookup"><span data-stu-id="299a9-160">Change the parameters for an application submitted using cURL</span></span>
<span data-ttu-id="299a9-161">Il comando seguente è un esempio di come modificare i parametri di configurazione per un'applicazione batch inviata tramite cURL.</span><span class="sxs-lookup"><span data-stu-id="299a9-161">Following command is an example of how to change the configuration parameters for a batch application that is submitted using using cURL.</span></span>

    curl -k -v -H 'Content-Type: application/json' -X POST -d '{"file":"<location of application jar file>", "className":"<the application class to execute>", "args":[<application parameters>], "numExecutors":10, "executorMemory":"2G", "executorCores":5' localhost:8998/batches

### <a name="how-do-i-change-these-parameters-on-a-spark-thrift-server"></a><span data-ttu-id="299a9-162">Come è possibile modificare questi parametri nel server Spark Thrift?</span><span class="sxs-lookup"><span data-stu-id="299a9-162">How do I change these parameters on a Spark Thrift Server?</span></span>
<span data-ttu-id="299a9-163">Il server Spark Thrift fornisce l'accesso JDBC/ODBC a un cluster Spark e viene usato per rispondere alle query di Spark SQL.</span><span class="sxs-lookup"><span data-stu-id="299a9-163">Spark Thrift Server provides JDBC/ODBC access to a Spark cluster and is used to service Spark SQL queries.</span></span> <span data-ttu-id="299a9-164">Strumenti come Power BI, Tableau e così via</span><span class="sxs-lookup"><span data-stu-id="299a9-164">Tools like Power BI, Tableau etc.</span></span> <span data-ttu-id="299a9-165">usano il protocollo ODBC per comunicare con il server Spark Thrift per eseguire query di Spark SQL come un'applicazione Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-165">use ODBC protocol to communicate with Spark Thrift Server to execute Spark SQL queries as a Spark Application.</span></span> <span data-ttu-id="299a9-166">Quando si crea un cluster Spark, vengono avviate due istanze del server Spark Thrift, una in ogni nodo head.</span><span class="sxs-lookup"><span data-stu-id="299a9-166">When a Spark cluster is created, two instances of the Spark Thrift Server are started, one on each head node.</span></span> <span data-ttu-id="299a9-167">Ogni Thrift Spark Server è visibile come un'applicazione Spark nell'interfaccia utente di YARN.</span><span class="sxs-lookup"><span data-stu-id="299a9-167">Each Spark Thrift Server is visible as a Spark application in the YARN UI.</span></span>

<span data-ttu-id="299a9-168">Il server Spark Thrift usa l'allocazione di executor dinamica di Spark e quindi non viene usato `spark.executor.instances` .</span><span class="sxs-lookup"><span data-stu-id="299a9-168">Spark Thrift Server uses Spark dynamic executor allocation and hence the `spark.executor.instances` is not used.</span></span> <span data-ttu-id="299a9-169">Il server Spark Thrift usa invece `spark.dynamicAllocation.minExecutors` e `spark.dynamicAllocation.maxExecutors` per specificare il numero di executor.</span><span class="sxs-lookup"><span data-stu-id="299a9-169">Instead, Spark Thrift Server uses `spark.dynamicAllocation.minExecutors` and `spark.dynamicAllocation.maxExecutors` to specify the executor count.</span></span> <span data-ttu-id="299a9-170">Per modificare le dimensioni degli executor, si usano i parametri di configurazione `spark.executor.cores` e `spark.executor.memory`.</span><span class="sxs-lookup"><span data-stu-id="299a9-170">The configuration parameters `spark.executor.cores` and `spark.executor.memory` is used to modify the executor size.</span></span> <span data-ttu-id="299a9-171">È possibile modificare questi parametri, come illustrato di seguito.</span><span class="sxs-lookup"><span data-stu-id="299a9-171">You can change these parameters as shown below.</span></span>

* <span data-ttu-id="299a9-172">Espandere la categoria **Advanced spark-thrift-sparkconf** per aggiornare i parametri `spark.dynamicAllocation.minExecutors`, `spark.dynamicAllocation.maxExecutors` e `spark.executor.memory`.</span><span class="sxs-lookup"><span data-stu-id="299a9-172">Expand the **Advanced spark-thrift-sparkconf** category to update the parameters `spark.dynamicAllocation.minExecutors`, `spark.dynamicAllocation.maxExecutors`, and `spark.executor.memory`.</span></span>

    ![Configurare il server Spark Thrift](./media/hdinsight-apache-spark-resource-manager/spark-thrift-server-1.png)    
* <span data-ttu-id="299a9-174">Espandere la categoria **Custom spark-thrift-sparkconf** per aggiornare il parametro `spark.executor.cores`.</span><span class="sxs-lookup"><span data-stu-id="299a9-174">Expand the **Custom spark-thrift-sparkconf** category to update the parameter `spark.executor.cores`.</span></span>

    ![Configurare il server Spark Thrift](./media/hdinsight-apache-spark-resource-manager/spark-thrift-server-2.png)

### <a name="how-do-i-change-the-driver-memory-of-the-spark-thrift-server"></a><span data-ttu-id="299a9-176">Come è possibile modificare la memoria del driver del server Spark Thrift?</span><span class="sxs-lookup"><span data-stu-id="299a9-176">How do I change the driver memory of the Spark Thrift Server?</span></span>
<span data-ttu-id="299a9-177">La memoria del driver del server Spark Thrift è configurata al 25% delle dimensioni della RAM nodo head, a condizione che le dimensioni totali della RAM del nodo head siano maggiori di 14 GB.</span><span class="sxs-lookup"><span data-stu-id="299a9-177">Spark Thrift Server driver memory is configured to 25% of the head node RAM size, provided the total RAM size of the head node is greater than 14GB.</span></span> <span data-ttu-id="299a9-178">Per modificare la configurazione della memoria del driver, è possibile usare l'interfaccia utente di Ambari, come illustrato di seguito.</span><span class="sxs-lookup"><span data-stu-id="299a9-178">You can use the Ambari UI to change the driver memory configuration, as shown below.</span></span>

* <span data-ttu-id="299a9-179">Nell'interfaccia utente di Ambari fare clic su **Spark**, **Configs** (Configurazioni), espandere **Advanced spark-env**, quindi specificare il valore per **spark_thrift_cmd_opts**.</span><span class="sxs-lookup"><span data-stu-id="299a9-179">From the Ambari UI click **Spark**, click **Configs**, expand **Advanced spark-env**, and then provide the value for **spark_thrift_cmd_opts**.</span></span>

    ![Configurare la RAM del server Spark Thrift](./media/hdinsight-apache-spark-resource-manager/spark-thrift-server-ram.png)

## <a name="i-do-not-use-bi-with-spark-cluster-how-do-i-take-the-resources-back"></a><span data-ttu-id="299a9-181">La funzionalità di Business Intelligence non è in uso con il cluster Spark.</span><span class="sxs-lookup"><span data-stu-id="299a9-181">I do not use BI with Spark cluster.</span></span> <span data-ttu-id="299a9-182">Come è possibile ripristinare le risorse?</span><span class="sxs-lookup"><span data-stu-id="299a9-182">How do I take the resources back?</span></span>
<span data-ttu-id="299a9-183">Poiché si usa l'assegnazione dinamica di Spark, le uniche risorse utilizzate dal server Thrift sono quelle per i due master applicazioni.</span><span class="sxs-lookup"><span data-stu-id="299a9-183">Since we use Spark dynamic allocation, the only resources that are consumed by thrift server are the resources for the two application masters.</span></span> <span data-ttu-id="299a9-184">Per recuperare tali risorse, è necessario arrestare i servizi del server Thrift in esecuzione nel cluster.</span><span class="sxs-lookup"><span data-stu-id="299a9-184">To reclaim these resources you must stop the Thrift Server services running on the cluster.</span></span>

1. <span data-ttu-id="299a9-185">Nel riquadro sinistro dell'interfaccia utente di Ambari fare clic su **Spark**.</span><span class="sxs-lookup"><span data-stu-id="299a9-185">From the Ambari UI, from the left pane, click **Spark**.</span></span>
2. <span data-ttu-id="299a9-186">Nella pagina successiva fare clic su **Server Spark Thrift**.</span><span class="sxs-lookup"><span data-stu-id="299a9-186">In the next page, click **Spark Thrift Servers**.</span></span>

    ![Riavviare il server Thrift](./media/hdinsight-apache-spark-resource-manager/restart-thrift-server-1.png)
3. <span data-ttu-id="299a9-188">Verranno visualizzati due nodi head in cui è in esecuzione il server Spark Thrift.</span><span class="sxs-lookup"><span data-stu-id="299a9-188">You should see the two headnodes on which the Spark Thrift Server is running.</span></span> <span data-ttu-id="299a9-189">Fare clic su uno dei nodi head.</span><span class="sxs-lookup"><span data-stu-id="299a9-189">Click one of the headnodes.</span></span>

    ![Riavviare il server Thrift](./media/hdinsight-apache-spark-resource-manager/restart-thrift-server-2.png)
4. <span data-ttu-id="299a9-191">Nella pagina successiva sono elencati tutti i servizi in esecuzione in quel nodo head.</span><span class="sxs-lookup"><span data-stu-id="299a9-191">The next page lists all the services running on that headnode.</span></span> <span data-ttu-id="299a9-192">Nell'elenco fare clic sul pulsante dell'elenco a discesa accanto al server Spark Thrift e quindi fare clic su **Stop**.</span><span class="sxs-lookup"><span data-stu-id="299a9-192">From the list click the drop-down button next to Spark Thrift Server, and then click **Stop**.</span></span>

    ![Riavviare il server Thrift](./media/hdinsight-apache-spark-resource-manager/restart-thrift-server-3.png)
5. <span data-ttu-id="299a9-194">Ripetere questi passaggi anche per l'altro nodo head.</span><span class="sxs-lookup"><span data-stu-id="299a9-194">Repeat these steps on the other headnode as well.</span></span>

## <a name="my-jupyter-notebooks-are-not-running-as-expected-how-can-i-restart-the-service"></a><span data-ttu-id="299a9-195">I notebook Jupyter non vengono eseguiti come previsto.</span><span class="sxs-lookup"><span data-stu-id="299a9-195">My Jupyter notebooks are not running as expected.</span></span> <span data-ttu-id="299a9-196">Come è possibile riavviare il servizio?</span><span class="sxs-lookup"><span data-stu-id="299a9-196">How can I restart the service?</span></span>
<span data-ttu-id="299a9-197">Avviare l'interfaccia utente Web di Ambari, come illustrato in precedenza.</span><span class="sxs-lookup"><span data-stu-id="299a9-197">Launch the Ambari Web UI as shown above.</span></span> <span data-ttu-id="299a9-198">Dal riquadro di spostamento sinistro fare clic su **Jupyter**, **Service Actions** (Azioni servizio) e quindi su **Restart All** (Riavvia tutto).</span><span class="sxs-lookup"><span data-stu-id="299a9-198">From the left navigation pane, click **Jupyter**, click **Service Actions**, and then click **Restart All**.</span></span> <span data-ttu-id="299a9-199">Verrà avviato il servizio Jupyter su tutti i nodi head.</span><span class="sxs-lookup"><span data-stu-id="299a9-199">This will start the Jupyter service on all the headnodes.</span></span>

    ![Restart Jupyter](./media/hdinsight-apache-spark-resource-manager/restart-jupyter.png "Restart Jupyter")

## <a name="how-do-i-know-if-i-am-running-out-of-resources"></a><span data-ttu-id="299a9-200">Rilevare l'esaurimento delle risorse</span><span class="sxs-lookup"><span data-stu-id="299a9-200">How do I know if I am running out of resources?</span></span>
<span data-ttu-id="299a9-201">Avviare l'interfaccia utente di Yarn come illustrato in precedenza.</span><span class="sxs-lookup"><span data-stu-id="299a9-201">Launch the Yarn UI as shown above.</span></span> <span data-ttu-id="299a9-202">Nella tabella Cluster Metrics (Metriche cluster) nella parte superiore della schermata, verificare i valori delle colonne **Memory Used** (Memoria in uso) e **Memory Total** (Memoria totale).</span><span class="sxs-lookup"><span data-stu-id="299a9-202">In Cluster Metrics table on top of the screen, check values of **Memory Used** and **Memory Total** columns.</span></span> <span data-ttu-id="299a9-203">Se i 2 valori sono molto simili, potrebbero non esserci risorse sufficienti per avviare l'applicazione successiva.</span><span class="sxs-lookup"><span data-stu-id="299a9-203">If the 2 values are very close, there might not be enough resources to start the next application.</span></span> <span data-ttu-id="299a9-204">Lo stesso vale per le colonne **VCores Used** (VCore in uso) e **VCores Total** (VCore totali).</span><span class="sxs-lookup"><span data-stu-id="299a9-204">The same applies to the **VCores Used** and **VCores Total** columns.</span></span> <span data-ttu-id="299a9-205">Se nella visualizzazione principale è presente un'applicazione con stato **ACCEPTED** (ACCETTATO) che non passa allo stato **RUNNING** (IN ESECUZIONE) o **FAILED** (NON RIUSCITO), ciò può anche indicare che l'applicazione non ha risorse sufficienti per l'avvio.</span><span class="sxs-lookup"><span data-stu-id="299a9-205">Also, in the main view, if there is an application stayed in **ACCEPTED** state and not transitioning into **RUNNING** nor **FAILED** state, this could also be an indication that it is not getting enough resources to start.</span></span>

    ![Resource Limit](./media/hdinsight-apache-spark-resource-manager/resource-limit.png "Resource Limit")

## <a name="how-do-i-kill-a-running-application-to-free-up-resource"></a><span data-ttu-id="299a9-206">Come è possibile terminare un'applicazione in esecuzione per liberare risorse?</span><span class="sxs-lookup"><span data-stu-id="299a9-206">How do I kill a running application to free up resource?</span></span>
1. <span data-ttu-id="299a9-207">Nell'interfaccia utente di Yarn, nel pannello a sinistra, fare clic su **In esecuzione**.</span><span class="sxs-lookup"><span data-stu-id="299a9-207">In the Yarn UI, from the left panel, click **Running**.</span></span> <span data-ttu-id="299a9-208">Dall'elenco delle applicazioni in esecuzione, determinare l'applicazione da terminare e fare clic sull'**ID**.</span><span class="sxs-lookup"><span data-stu-id="299a9-208">From the list of running applications, determine the application to be killed and click on the **ID**.</span></span>

    <span data-ttu-id="299a9-209">![Terminare App1](./media/hdinsight-apache-spark-resource-manager/kill-app1.png "Terminare App1")</span><span class="sxs-lookup"><span data-stu-id="299a9-209">![Kill App1](./media/hdinsight-apache-spark-resource-manager/kill-app1.png "Kill App1")</span></span>

2. <span data-ttu-id="299a9-210">Fare clic su **Kill Application** (Termina applicazione) nella parte superiore destra, quindi fare clic su **OK**.</span><span class="sxs-lookup"><span data-stu-id="299a9-210">Click **Kill Application** on the top right corner, then click **OK**.</span></span>

    <span data-ttu-id="299a9-211">![Terminare App2](./media/hdinsight-apache-spark-resource-manager/kill-app2.png "Terminare App2")</span><span class="sxs-lookup"><span data-stu-id="299a9-211">![Kill App2](./media/hdinsight-apache-spark-resource-manager/kill-app2.png "Kill App2")</span></span>

## <a name="see-also"></a><span data-ttu-id="299a9-212">Vedere anche</span><span class="sxs-lookup"><span data-stu-id="299a9-212">See also</span></span>
* [<span data-ttu-id="299a9-213">Tenere traccia ed eseguire il debug di processi in esecuzione nel cluster Apache Spark in Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="299a9-213">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)

### <a name="for-data-analysts"></a><span data-ttu-id="299a9-214">Per gli analisti dei dati</span><span class="sxs-lookup"><span data-stu-id="299a9-214">For data analysts</span></span>

* [<span data-ttu-id="299a9-215">Spark con Machine Learning: utilizzare Spark in HDInsight per l'analisi della temperatura di compilazione utilizzando dati HVAC</span><span class="sxs-lookup"><span data-stu-id="299a9-215">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="299a9-216">Spark con Machine Learning: usare Spark in HDInsight per prevedere i risultati del controllo degli alimenti</span><span class="sxs-lookup"><span data-stu-id="299a9-216">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="299a9-217">Analisi dei log del sito Web mediante Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="299a9-217">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)
* [<span data-ttu-id="299a9-218">Application Insight telemetry data analysis using Spark in HDInsight (Analisi dei dati di telemetria di Application Insights con Spark in HDInsight)</span><span class="sxs-lookup"><span data-stu-id="299a9-218">Application Insight telemetry data analysis using Spark in HDInsight</span></span>](hdinsight-spark-analyze-application-insight-logs.md)
* [<span data-ttu-id="299a9-219">Usare Caffe in Azure HDInsight Spark per l'apprendimento avanzato distribuito</span><span class="sxs-lookup"><span data-stu-id="299a9-219">Use Caffe on Azure HDInsight Spark for distributed deep learning</span></span>](hdinsight-deep-learning-caffe-spark.md)

### <a name="for-spark-developers"></a><span data-ttu-id="299a9-220">Per gli sviluppatori di Spark</span><span class="sxs-lookup"><span data-stu-id="299a9-220">For Spark developers</span></span>

* [<span data-ttu-id="299a9-221">Creare un'applicazione autonoma con Scala</span><span class="sxs-lookup"><span data-stu-id="299a9-221">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="299a9-222">Eseguire processi in modalità remota in un cluster Spark usando Livy</span><span class="sxs-lookup"><span data-stu-id="299a9-222">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)
* [<span data-ttu-id="299a9-223">Usare il plug-in degli strumenti HDInsight per IntelliJ IDEA per creare e inviare applicazioni Spark in Scala</span><span class="sxs-lookup"><span data-stu-id="299a9-223">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="299a9-224">Streaming Spark: usare Spark in HDInsight per la creazione di applicazioni di streaming in tempo reale</span><span class="sxs-lookup"><span data-stu-id="299a9-224">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="299a9-225">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely (Usare il plug-in Strumenti HDInsight per IntelliJ IDEA per eseguire il debug di applicazioni Spark in remoto)</span><span class="sxs-lookup"><span data-stu-id="299a9-225">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="299a9-226">Usare i notebook di Zeppelin con un cluster Spark in HDInsight</span><span class="sxs-lookup"><span data-stu-id="299a9-226">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="299a9-227">Kernel disponibili per notebook di Jupyter nel cluster Spark per HDInsight</span><span class="sxs-lookup"><span data-stu-id="299a9-227">Kernels available for Jupyter notebook in Spark cluster for HDInsight</span></span>](hdinsight-apache-spark-jupyter-notebook-kernels.md)
* [<span data-ttu-id="299a9-228">Usare pacchetti esterni con i notebook Jupyter</span><span class="sxs-lookup"><span data-stu-id="299a9-228">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="299a9-229">Installare Jupyter Notebook nel computer e connetterlo a un cluster HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="299a9-229">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)
