---
title: Guida al linguaggio di specifica Net# delle reti neurali | Microsoft Docs
description: 'Sintassi per la Net # neural networks lingua specifica, con relativi esempi di come creare un modello di rete neurale personalizzata in Microsoft Azure Machine Learning tramite Net #'
services: machine-learning
documentationcenter: 
author: jeannt
manager: jhubbard
editor: cgronlun
ms.assetid: cfd1454b-47df-4745-b064-ce5f9b3be303
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/31/2017
ms.author: jeannt
ms.openlocfilehash: 965c60ffde55041cc3864d06d81f5590c7ea1c11
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 07/11/2017
---
# <a name="guide-to-net-neural-network-specification-language-for-azure-machine-learning"></a><span data-ttu-id="c8531-103">Guida al linguaggio di specifica Net# delle reti neurali per Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="c8531-103">Guide to Net# neural network specification language for Azure Machine Learning</span></span>
## <a name="overview"></a><span data-ttu-id="c8531-104">Panoramica</span><span class="sxs-lookup"><span data-stu-id="c8531-104">Overview</span></span>
<span data-ttu-id="c8531-105">Net# è un linguaggio sviluppato da Microsoft e usato per definire le architetture di reti neurali.</span><span class="sxs-lookup"><span data-stu-id="c8531-105">Net# is a language developed by Microsoft that is used to define neural network architectures.</span></span> <span data-ttu-id="c8531-106">È possibile usare Net# in moduli di reti neurali in Microsoft Azure Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="c8531-106">You can use Net# in neural network modules in Microsoft Azure Machine Learning.</span></span>

<!-- This function doesn't currentlyappear in the MicrosoftML documentation. If it is added in a future update, we can uncomment this text.

, or in the `rxNeuralNetwork()` function in [MicrosoftML](https://msdn.microsoft.com/microsoft-r/microsoftml/microsoftml). 

-->

<span data-ttu-id="c8531-107">Questo articolo illustra i concetti base necessari per lo sviluppo di una rete neurale personalizzata:</span><span class="sxs-lookup"><span data-stu-id="c8531-107">In this article, you will learn basic concepts needed to develop a custom neural network:</span></span> 

* <span data-ttu-id="c8531-108">Requisiti relativi alla rete neurale e come definire i componenti principali</span><span class="sxs-lookup"><span data-stu-id="c8531-108">Neural network requirements and how to define the primary components</span></span>
* <span data-ttu-id="c8531-109">Sintassi e parole chiave del linguaggio di specifica Net#</span><span class="sxs-lookup"><span data-stu-id="c8531-109">The syntax and keywords of the Net# specification language</span></span>
* <span data-ttu-id="c8531-110">Esempi delle reti neurali personalizzati creati tramite Net #</span><span class="sxs-lookup"><span data-stu-id="c8531-110">Examples of custom neural networks created using Net#</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

## <a name="neural-network-basics"></a><span data-ttu-id="c8531-111">Nozioni di base sulla rete neurale</span><span class="sxs-lookup"><span data-stu-id="c8531-111">Neural network basics</span></span>
<span data-ttu-id="c8531-112">Una struttura di rete neurale è composta da ***nodi*** organizzati in ***livelli*** e ***connessioni*** ponderate (o ***bordi***) tra i nodi.</span><span class="sxs-lookup"><span data-stu-id="c8531-112">A neural network structure consists of ***nodes*** that are organized in ***layers***, and weighted ***connections*** (or ***edges***) between the nodes.</span></span> <span data-ttu-id="c8531-113">Le connessioni sono direzionali e ognuna ha un nodo di ***origine*** e un nodo di ***destinazione***.</span><span class="sxs-lookup"><span data-stu-id="c8531-113">The connections are directional, and each connection has a ***source*** node and a ***destination*** node.</span></span>  

<span data-ttu-id="c8531-114">Ogni ***livello di cui è possibile eseguire il training*** (livello nascosto o di output) ha una o più ***aggregazioni di connessioni***.</span><span class="sxs-lookup"><span data-stu-id="c8531-114">Each ***trainable layer*** (a hidden or an output layer) has one or more ***connection bundles***.</span></span> <span data-ttu-id="c8531-115">Un'aggregazione di connessioni è costituita da un livello di origine e da una specifica delle connessioni provenienti da quel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-115">A connection bundle consists of a source layer and a specification of the connections from that source layer.</span></span> <span data-ttu-id="c8531-116">Tutte le connessioni in una determinata aggregazione condividono lo stesso ***livello di origine*** e lo stesso ***livello di destinazione***.</span><span class="sxs-lookup"><span data-stu-id="c8531-116">All the connections in a given bundle share the same ***source layer*** and the same ***destination layer***.</span></span> <span data-ttu-id="c8531-117">In Net# un'aggregazione di connessioni è considerata come appartenente al livello di destinazione dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-117">In Net#, a connection bundle is considered as belonging to the bundle's destination layer.</span></span>  

<span data-ttu-id="c8531-118">Net# supporta diversi tipi di aggregazioni di connessioni, permettendo quindi di personalizzare il modo in cui gli input sono mappati per i livelli nascosti e agli output.</span><span class="sxs-lookup"><span data-stu-id="c8531-118">Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.</span></span>   

<span data-ttu-id="c8531-119">L'aggregazione predefinita o standard è un'**aggregazione completa**, in cui ogni nodo del livello di origine è connesso a tutti i nodi del livello di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-119">The default or standard bundle is a **full bundle**, in which each node in the source layer is connected to every node in the destination layer.</span></span>  

<span data-ttu-id="c8531-120">Net# supporta anche i quattro tipi seguenti di aggregazioni di connessioni avanzate:</span><span class="sxs-lookup"><span data-stu-id="c8531-120">Additionally, Net# supports the following four kinds of advanced connection bundles:</span></span>  

* <span data-ttu-id="c8531-121">**Aggregazioni filtrate**.</span><span class="sxs-lookup"><span data-stu-id="c8531-121">**Filtered bundles**.</span></span> <span data-ttu-id="c8531-122">L'utente può definire un predicato usando le posizioni del nodo di livelli di origine e del nodo di livelli di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-122">The user can define a predicate by using the locations of the source layer node and the destination layer node.</span></span> <span data-ttu-id="c8531-123">I nodi vengono connessi se il predicato è True.</span><span class="sxs-lookup"><span data-stu-id="c8531-123">Nodes are connected whenever the predicate is True.</span></span>
* <span data-ttu-id="c8531-124">**Aggregazioni convoluzionali**.</span><span class="sxs-lookup"><span data-stu-id="c8531-124">**Convolutional bundles**.</span></span> <span data-ttu-id="c8531-125">L'utente può definire piccoli intorni di nodi nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-125">The user can define small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="c8531-126">Ogni nodo nel livello di destinazione è connesso a un intorno di nodi nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-126">Each node in the destination layer is connected to one neighborhood of nodes in the source layer.</span></span>
* <span data-ttu-id="c8531-127">**Aggregazioni di pooling** e **aggregazioni di normalizzazione delle risposte**.</span><span class="sxs-lookup"><span data-stu-id="c8531-127">**Pooling bundles** and **Response normalization bundles**.</span></span> <span data-ttu-id="c8531-128">Sono simili alle aggregazioni convoluzionali, in quanto l'utente definisce piccoli intorni di nodi nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-128">These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="c8531-129">La differenza è che i pesi dei bordi in questi pacchetti non sono addestrabili.</span><span class="sxs-lookup"><span data-stu-id="c8531-129">The difference is that the weights of the edges in these bundles are not trainable.</span></span> <span data-ttu-id="c8531-130">Infatti, una funzione predefinita viene applicata ai valori del nodo di origine per determinare il valore del nodo di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-130">Instead, a predefined function is applied to the source node values to determine the destination node value.</span></span>  

<span data-ttu-id="c8531-131">L'uso di Net# per definire la struttura di una rete neurale rende possibile la definizione di strutture complesse quali reti neurali profonde o convoluzioni di dimensioni arbitrarie, che notoriamente migliorano l'apprendimento relativo a dati quali immagini, audio e video.</span><span class="sxs-lookup"><span data-stu-id="c8531-131">Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.</span></span>  

## <a name="supported-customizations"></a><span data-ttu-id="c8531-132">Personalizzazioni supportate</span><span class="sxs-lookup"><span data-stu-id="c8531-132">Supported customizations</span></span>
<span data-ttu-id="c8531-133">L'architettura dei modelli di rete neurale creati in Azure Machine Learning può essere ampiamente personalizzata tramite Net#.</span><span class="sxs-lookup"><span data-stu-id="c8531-133">The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.</span></span> <span data-ttu-id="c8531-134">È possibile:</span><span class="sxs-lookup"><span data-stu-id="c8531-134">You can:</span></span>  

* <span data-ttu-id="c8531-135">Creare livelli nascosti e controllare il numero di nodi in ogni livello.</span><span class="sxs-lookup"><span data-stu-id="c8531-135">Create hidden layers and control the number of nodes in each layer.</span></span>
* <span data-ttu-id="c8531-136">Specificare la modalità di connessione reciproca dei livelli.</span><span class="sxs-lookup"><span data-stu-id="c8531-136">Specify how layers are to be connected to each other.</span></span>
* <span data-ttu-id="c8531-137">Definire strutture di connettività speciali, ad esempio aggregazioni di convoluzioni e per la condivisione dei pesi.</span><span class="sxs-lookup"><span data-stu-id="c8531-137">Define special connectivity structures, such as convolutions and weight sharing bundles.</span></span>
* <span data-ttu-id="c8531-138">Specificare diverse funzioni di attivazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-138">Specify different activation functions.</span></span>  

<span data-ttu-id="c8531-139">Per informazioni dettagliate sulla sintassi del linguaggio di specifica, vedere [Specifiche di struttura](#Structure-specifications).</span><span class="sxs-lookup"><span data-stu-id="c8531-139">For details of the specification language syntax, see [Structure Specification](#Structure-specifications).</span></span>  

<span data-ttu-id="c8531-140">Per esempi di definizione di reti neurali per alcune attività comuni di Machine Learning, dalle più semplici alle più complesse, vedere [Esempi](#Examples-of-Net#-usage).</span><span class="sxs-lookup"><span data-stu-id="c8531-140">For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see [Examples](#Examples-of-Net#-usage).</span></span>  

## <a name="general-requirements"></a><span data-ttu-id="c8531-141">Requisiti generali</span><span class="sxs-lookup"><span data-stu-id="c8531-141">General requirements</span></span>
* <span data-ttu-id="c8531-142">Devono essere esattamente disponibili un livello di output, almeno un livello di input e zero o più livelli nascosti.</span><span class="sxs-lookup"><span data-stu-id="c8531-142">There must be exactly one output layer, at least one input layer, and zero or more hidden layers.</span></span> 
* <span data-ttu-id="c8531-143">Ogni livello ha un numero fisso di nodi, disposti concettualmente in una matrice rettangolare di dimensioni arbitrarie.</span><span class="sxs-lookup"><span data-stu-id="c8531-143">Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.</span></span> 
* <span data-ttu-id="c8531-144">I livelli di input non sono associati a parametri sottoposti a training e rappresentano il punto in cui i dati delle istanze entrano nella rete.</span><span class="sxs-lookup"><span data-stu-id="c8531-144">Input layers have no associated trained parameters and represent the point where instance data enters the network.</span></span> 
* <span data-ttu-id="c8531-145">I livelli sottoponibili a training, ovvero i livelli nascosti e di output hanno parametri sottoposti a training associati, noti come pesi e distorsioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-145">Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.</span></span> 
* <span data-ttu-id="c8531-146">I nodi di origine e di destinazione devono trovarsi in livelli separati.</span><span class="sxs-lookup"><span data-stu-id="c8531-146">The source and destination nodes must be in separate layers.</span></span> 
* <span data-ttu-id="c8531-147">La connessione deve essere aciclica, ovvero non può essere presente una catena di connessioni che riporta al nodo di origine iniziale.</span><span class="sxs-lookup"><span data-stu-id="c8531-147">Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.</span></span>
* <span data-ttu-id="c8531-148">Il livello di output non può essere un livello di origine di un'aggregazione di connessioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-148">The output layer cannot be a source layer of a connection bundle.</span></span>  

## <a name="structure-specifications"></a><span data-ttu-id="c8531-149">Specifiche di struttura</span><span class="sxs-lookup"><span data-stu-id="c8531-149">Structure specifications</span></span>
<span data-ttu-id="c8531-150">La specifica della struttura di una rete neurale è composta da tre sezioni: la **dichiarazione delle costanti**, la **dichiarazione dei livelli** e la **dichiarazione delle connessioni**.</span><span class="sxs-lookup"><span data-stu-id="c8531-150">A neural network structure specification is composed of three sections: the **constant declaration**, the **layer declaration**, the **connection declaration**.</span></span> <span data-ttu-id="c8531-151">È anche disponibile una sezione facoltativa denominata **dichiarazione delle condivisioni**.</span><span class="sxs-lookup"><span data-stu-id="c8531-151">There is also an optional **share declaration** section.</span></span> <span data-ttu-id="c8531-152">Le sezioni possono essere specificate in qualsiasi ordine.</span><span class="sxs-lookup"><span data-stu-id="c8531-152">The sections can be specified in any order.</span></span>  

## <a name="constant-declaration"></a><span data-ttu-id="c8531-153">Dichiarazione della costante</span><span class="sxs-lookup"><span data-stu-id="c8531-153">Constant declaration</span></span>
<span data-ttu-id="c8531-154">Una dichiarazione della costante è facoltativa.</span><span class="sxs-lookup"><span data-stu-id="c8531-154">A constant declaration is optional.</span></span> <span data-ttu-id="c8531-155">Fornisce un mezzo per definire i valori usati in una qualunque posizione nella definizione della rete neurale.</span><span class="sxs-lookup"><span data-stu-id="c8531-155">It provides a means to define values used elsewhere in the neural network definition.</span></span> <span data-ttu-id="c8531-156">L'istruzione di dichiarazione è costituita da un identificatore seguito da un segno di uguale e un'espressione valore.</span><span class="sxs-lookup"><span data-stu-id="c8531-156">The declaration statement consists of an identifier followed by an equal sign and a value expression.</span></span>   

<span data-ttu-id="c8531-157">Ad esempio, l'istruzione seguente definisce una costante **x**:</span><span class="sxs-lookup"><span data-stu-id="c8531-157">For example, the following statement defines a constant **x**:</span></span>  

    Const X = 28;  

<span data-ttu-id="c8531-158">Per definire contemporaneamente due o più costanti, racchiudere i nomi e i valori degli identificatori tra parentesi graffe e separarle da punti e virgola.</span><span class="sxs-lookup"><span data-stu-id="c8531-158">To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.</span></span> <span data-ttu-id="c8531-159">ad esempio:</span><span class="sxs-lookup"><span data-stu-id="c8531-159">For example:</span></span>  

    Const { X = 28; Y = 4; }  

<span data-ttu-id="c8531-160">La parte destra di ogni espressione di assegnazione può essere costituita da un valore Integer, un numero reale, un valore booleano (true/false) o un'espressione matematica.</span><span class="sxs-lookup"><span data-stu-id="c8531-160">The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.</span></span> <span data-ttu-id="c8531-161">ad esempio:</span><span class="sxs-lookup"><span data-stu-id="c8531-161">For example:</span></span>  

    Const { X = 17 * 2; Y = true; }  

## <a name="layer-declaration"></a><span data-ttu-id="c8531-162">Dichiarazione dei livelli</span><span class="sxs-lookup"><span data-stu-id="c8531-162">Layer declaration</span></span>
<span data-ttu-id="c8531-163">La dichiarazione di livello è obbligatoria.</span><span class="sxs-lookup"><span data-stu-id="c8531-163">The layer declaration is required.</span></span> <span data-ttu-id="c8531-164">Definisce le dimensioni e l'origine del livello, inclusi il raggruppamento di connessione e gli attributi.</span><span class="sxs-lookup"><span data-stu-id="c8531-164">It defines the size and source of the layer, including its connection bundles and attributes.</span></span> <span data-ttu-id="c8531-165">L'istruzione della dichiarazione inizia con il nome del livello (di input, nascosto o di output), seguito dalle dimensioni del livello (una tupla di valori Integer positivi).</span><span class="sxs-lookup"><span data-stu-id="c8531-165">The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).</span></span> <span data-ttu-id="c8531-166">ad esempio:</span><span class="sxs-lookup"><span data-stu-id="c8531-166">For example:</span></span>  

    input Data auto;
    hidden Hidden[5,20] from Data all;
    output Result[2] from Hidden all;  

* <span data-ttu-id="c8531-167">Il prodotto delle dimensioni è il numero di nodi nel livello.</span><span class="sxs-lookup"><span data-stu-id="c8531-167">The product of the dimensions is the number of nodes in the layer.</span></span> <span data-ttu-id="c8531-168">In questo esempio ci sono due dimensioni [5,20]. Ciò significa che nel livello ci sono 100 nodi.</span><span class="sxs-lookup"><span data-stu-id="c8531-168">In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.</span></span>
* <span data-ttu-id="c8531-169">I livelli possono essere dichiarati in qualsiasi ordine, con una eccezione: se sono stati definiti più livelli di input, l'ordine in cui vengono dichiarati deve corrispondere all'ordine delle funzionalità nei dati di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-169">The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.</span></span>  

<span data-ttu-id="c8531-170">Per specificare che il numero di nodi in un livello deve essere determinato automaticamente, usare la parola chiave **auto**.</span><span class="sxs-lookup"><span data-stu-id="c8531-170">To specify that the number of nodes in a layer be determined automatically, use the **auto** keyword.</span></span> <span data-ttu-id="c8531-171">La parola chiave **auto** ha effetti diversi, a seconda del livello:</span><span class="sxs-lookup"><span data-stu-id="c8531-171">The **auto** keyword has different effects, depending on the layer:</span></span>  

* <span data-ttu-id="c8531-172">Nella dichiarazione di un livello di input il numero di nodi corrisponde al numero di funzionalità nei dati di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-172">In an input layer declaration, the number of nodes is the number of features in the input data.</span></span>
* <span data-ttu-id="c8531-173">In una dichiarazione di livello nascosto, il numero di nodi corrisponde al numero specificato dal valore del parametro per il **numero di nodi nascosti**.</span><span class="sxs-lookup"><span data-stu-id="c8531-173">In a hidden layer declaration, the number of nodes is the number that is specified by the parameter value for **Number of hidden nodes**.</span></span> 
* <span data-ttu-id="c8531-174">Nella dichiarazione di un livello di output il numero di nodi corrisponde a 2 per la classificazione a due classi, 1 per la regressione ed è uguale al numero di nodi di output per la classificazione a più classi.</span><span class="sxs-lookup"><span data-stu-id="c8531-174">In an output layer declaration, the number of nodes is 2 for two-class classification, 1 for regression, and equal to the number of output nodes for multiclass classification.</span></span>   

<span data-ttu-id="c8531-175">Ad esempio, la definizione di rete seguente permette la determinazione automatica delle dimensioni di tutti i livelli:</span><span class="sxs-lookup"><span data-stu-id="c8531-175">For example, the following network definition allows the size of all layers to be automatically determined:</span></span>  

    input Data auto;
    hidden Hidden auto from Data all;
    output Result auto from Hidden all;  


<span data-ttu-id="c8531-176">Una dichiarazione di livelli per un livello di cui è possibile eseguire il training, ovvero i livelli nascosti o di output, può includere facoltativamente la funzione di output, definita anche funzione di attivazione, che assume il valore predefinito **sigmoid** per i modelli di classificazione e **linear** per i modelli di regressione.</span><span class="sxs-lookup"><span data-stu-id="c8531-176">A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to **sigmoid** for classification models, and **linear** for regression models.</span></span> <span data-ttu-id="c8531-177">(Anche se si usa l'impostazione predefinita, è possibile dichiarare in modo esplicito la funzione di attivazione, se lo si desidera per maggiore chiarezza.)</span><span class="sxs-lookup"><span data-stu-id="c8531-177">(Even if you use the default, you can explicitly state the activation function, if desired for clarity.)</span></span>

<span data-ttu-id="c8531-178">Sono supportate le funzioni di output seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-178">The following output functions are supported:</span></span>  

* <span data-ttu-id="c8531-179">sigmoid</span><span class="sxs-lookup"><span data-stu-id="c8531-179">sigmoid</span></span>
* <span data-ttu-id="c8531-180">linear</span><span class="sxs-lookup"><span data-stu-id="c8531-180">linear</span></span>
* <span data-ttu-id="c8531-181">softmax</span><span class="sxs-lookup"><span data-stu-id="c8531-181">softmax</span></span>
* <span data-ttu-id="c8531-182">rlinear</span><span class="sxs-lookup"><span data-stu-id="c8531-182">rlinear</span></span>
* <span data-ttu-id="c8531-183">square</span><span class="sxs-lookup"><span data-stu-id="c8531-183">square</span></span>
* <span data-ttu-id="c8531-184">sqrt</span><span class="sxs-lookup"><span data-stu-id="c8531-184">sqrt</span></span>
* <span data-ttu-id="c8531-185">srlinear</span><span class="sxs-lookup"><span data-stu-id="c8531-185">srlinear</span></span>
* <span data-ttu-id="c8531-186">abs</span><span class="sxs-lookup"><span data-stu-id="c8531-186">abs</span></span>
* <span data-ttu-id="c8531-187">tanh</span><span class="sxs-lookup"><span data-stu-id="c8531-187">tanh</span></span> 
* <span data-ttu-id="c8531-188">brlinear</span><span class="sxs-lookup"><span data-stu-id="c8531-188">brlinear</span></span>  

<span data-ttu-id="c8531-189">Ad esempio, la dichiarazione seguente usa la funzione **softmax**:</span><span class="sxs-lookup"><span data-stu-id="c8531-189">For example, the following declaration uses the **softmax** function:</span></span>  

    output Result [100] softmax from Hidden all;  

## <a name="connection-declaration"></a><span data-ttu-id="c8531-190">Dichiarazione della connessione</span><span class="sxs-lookup"><span data-stu-id="c8531-190">Connection declaration</span></span>
<span data-ttu-id="c8531-191">Immediatamente dopo aver definito il livello sottoponibile a training, è necessario dichiarare connessioni tra i livelli definiti.</span><span class="sxs-lookup"><span data-stu-id="c8531-191">Immediately after defining the trainable layer, you must declare connections among the layers you have defined.</span></span> <span data-ttu-id="c8531-192">La dichiarazione di aggregazione delle connessioni inizia con la parola chiave **from**, seguita dal nome del livello di origine dell'aggregazione e dal tipo di aggregazione di connessioni da creare.</span><span class="sxs-lookup"><span data-stu-id="c8531-192">The connection bundle declaration starts with the keyword **from**, followed by the name of the bundle's source layer and the kind of connection bundle to create.</span></span>   

<span data-ttu-id="c8531-193">Sono attualmente supportati cinque tipi di aggregazioni di connessioni:</span><span class="sxs-lookup"><span data-stu-id="c8531-193">Currently, five kinds of connection bundles are supported:</span></span>  

* <span data-ttu-id="c8531-194">Aggregazioni **complete**, indicate dalla parola chiave **all**</span><span class="sxs-lookup"><span data-stu-id="c8531-194">**Full** bundles, indicated by the keyword **all**</span></span>
* <span data-ttu-id="c8531-195">Aggregazioni **filtrate**, indicate dalla parola chiave **where**, seguita da un'espressione del predicato</span><span class="sxs-lookup"><span data-stu-id="c8531-195">**Filtered** bundles, indicated by the keyword **where**, followed by a predicate expression</span></span>
* <span data-ttu-id="c8531-196">Aggregazioni **convoluzionali**, indicate dalla parola chiave **convolve**, seguite dagli attributi di convoluzione</span><span class="sxs-lookup"><span data-stu-id="c8531-196">**Convolutional** bundles, indicated by the keyword **convolve**, followed by the convolution attributes</span></span>
* <span data-ttu-id="c8531-197">Aggregazioni di **pooling**, indicate dalle parole chiave **max pool** o **mean pool**</span><span class="sxs-lookup"><span data-stu-id="c8531-197">**Pooling** bundles, indicated by the keywords **max pool** or **mean pool**</span></span>
* <span data-ttu-id="c8531-198">Aggregazioni di **normalizzazione delle risposte**, indicate dalla parola chiave **response norm**</span><span class="sxs-lookup"><span data-stu-id="c8531-198">**Response normalization** bundles, indicated by the keyword **response norm**</span></span>      

## <a name="full-bundles"></a><span data-ttu-id="c8531-199">Aggregazioni complete</span><span class="sxs-lookup"><span data-stu-id="c8531-199">Full bundles</span></span>
<span data-ttu-id="c8531-200">Un'aggregazione completa di connessioni include una connessione da ogni nodo del livello di origine verso ogni nodo del livello di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-200">A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.</span></span> <span data-ttu-id="c8531-201">Si tratta del tipo di connessione di rete predefinito.</span><span class="sxs-lookup"><span data-stu-id="c8531-201">This is the default network connection type.</span></span>  

## <a name="filtered-bundles"></a><span data-ttu-id="c8531-202">Aggregazioni filtrate</span><span class="sxs-lookup"><span data-stu-id="c8531-202">Filtered bundles</span></span>
<span data-ttu-id="c8531-203">Una specifica di aggregazione di connessioni filtrata include un predicato, espresso sintatticamente in modo analogo a un'espressione lambda in C#.</span><span class="sxs-lookup"><span data-stu-id="c8531-203">A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.</span></span> <span data-ttu-id="c8531-204">L'esempio seguente definisce due aggregazioni filtrate:</span><span class="sxs-lookup"><span data-stu-id="c8531-204">The following example defines two filtered bundles:</span></span>  

    input Pixels [10, 20];
    hidden ByRow[10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol[5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;  

* <span data-ttu-id="c8531-205">Nel predicato per *ByRow*, **s** è un parametro che rappresenta un indice nella matrice rettangolare di nodi del livello di input *Pixels* e **d** è un parametro che rappresenta un indice nella matrice di nodi del livello nascosto *ByRow*.</span><span class="sxs-lookup"><span data-stu-id="c8531-205">In the predicate for *ByRow*, **s** is a parameter representing an index into the rectangular array of nodes of the input layer, *Pixels*, and **d** is a parameter representing an index into the array of nodes of the hidden layer, *ByRow*.</span></span> <span data-ttu-id="c8531-206">Il tipo di **s** e **d** è una tupla di valori interi di lunghezza due.</span><span class="sxs-lookup"><span data-stu-id="c8531-206">The type of both **s** and **d** is a tuple of integers of length two.</span></span> <span data-ttu-id="c8531-207">Concettualmente, **s** include tutte le coppie di valori interi con *0 <= s[0] < 10* e *0 <= s[1] < 20* e **d** include tutte le coppie di valori interi con *0 <= d[0] < 10* e *0 <= d[1] < 12*.</span><span class="sxs-lookup"><span data-stu-id="c8531-207">Conceptually, **s** ranges over all pairs of integers with *0 <= s[0] < 10* and *0 <= s[1] < 20*, and **d** ranges over all pairs of integers, with *0 <= d[0] < 10* and *0 <= d[1] < 12*.</span></span> 
* <span data-ttu-id="c8531-208">Una condizione è presente nella parte destra dell'espressione del predicato.</span><span class="sxs-lookup"><span data-stu-id="c8531-208">On the right-hand side of the predicate expression, there is a condition.</span></span> <span data-ttu-id="c8531-209">In questo esempio, per ogni valore di **s** e **d** tale da rendere la condizione true, c'è un bordo dal nodo del livello di origine al nodo del livello di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-209">In this example, for every value of **s** and **d** such that the condition is True, there is an edge from the source layer node to the destination layer node.</span></span> <span data-ttu-id="c8531-210">Questa espressione di filtro indica quindi che l'aggregazione include una connessione dal nodo definito da **s** al nodo definito da **d** in tutti i casi in cui s[0] è uguale a d[0].</span><span class="sxs-lookup"><span data-stu-id="c8531-210">Thus, this filter expression indicates that the bundle includes a connection from the node defined by **s** to the node defined by **d** in all cases where s[0] is equal to d[0].</span></span>  

<span data-ttu-id="c8531-211">È facoltativamente possibile specificare un insieme di pesi per un'aggregazione filtrata.</span><span class="sxs-lookup"><span data-stu-id="c8531-211">Optionally, you can specify a set of weights for a filtered bundle.</span></span> <span data-ttu-id="c8531-212">Il valore dell'attributo **Weights** deve essere una tupla di valori a virgola mobile, la cui lunghezza corrisponde al numero di connessioni definite dall'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-212">The value for the **Weights** attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.</span></span> <span data-ttu-id="c8531-213">Per impostazione predefinita, i pesi sono generati in modo casuale.</span><span class="sxs-lookup"><span data-stu-id="c8531-213">By default, weights are randomly generated.</span></span>  

<span data-ttu-id="c8531-214">I valori dei pesi sono raggruppati in base all'indice dei nodi di destinazione,</span><span class="sxs-lookup"><span data-stu-id="c8531-214">Weight values are grouped by the destination node index.</span></span> <span data-ttu-id="c8531-215">ovvero, se il primo nodo di destinazione è connesso ai nodi di origine K, i primi elementi *K* della tupla **Weights** rappresentano i pesi per il primo nodo di destinazione nell'ordine dell'indice di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-215">That is, if the first destination node is connected to K source nodes, the first *K* elements of the **Weights** tuple are the weights for the first destination node, in source index order.</span></span> <span data-ttu-id="c8531-216">La stessa procedura è applicabile ai nodi di destinazione rimanenti.</span><span class="sxs-lookup"><span data-stu-id="c8531-216">The same applies for the remaining destination nodes.</span></span>  

<span data-ttu-id="c8531-217">È possibile specificare i pesi direttamente come valori costanti.</span><span class="sxs-lookup"><span data-stu-id="c8531-217">It's possible to specify weights directly as constant values.</span></span> <span data-ttu-id="c8531-218">Se ad esempio si sono appresi in precedenza i pesi, è possibile specificarli come costanti usando questa sintassi:</span><span class="sxs-lookup"><span data-stu-id="c8531-218">For example, if you learned the weights previously, you can specify them as constants using this syntax:</span></span>

    const Weights_1 = [0.0188045055, 0.130500451, ...]


## <a name="convolutional-bundles"></a><span data-ttu-id="c8531-219">Aggregazioni convoluzionali</span><span class="sxs-lookup"><span data-stu-id="c8531-219">Convolutional bundles</span></span>
<span data-ttu-id="c8531-220">Quando i dati di training hanno una struttura omogenea, le connessioni convoluzionali vengono solitamente usate per ottenere funzionalità di livello elevato dei dati.</span><span class="sxs-lookup"><span data-stu-id="c8531-220">When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.</span></span> <span data-ttu-id="c8531-221">Ad esempio, nei dati di tipo immagine, audio o video è possibile che la dimensionalità spaziale o temporale sia abbastanza uniforme.</span><span class="sxs-lookup"><span data-stu-id="c8531-221">For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.</span></span>  

<span data-ttu-id="c8531-222">Le aggregazioni convoluzionali usano**kernels** rettangolari inseriti nelle dimensioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-222">Convolutional bundles employ rectangular **kernels** that are slid through the dimensions.</span></span> <span data-ttu-id="c8531-223">Essenzialmente, ogni kernel definisce un insieme di pesi applicati nei vicinati locali, indicati come **applicazioni di kernel**.</span><span class="sxs-lookup"><span data-stu-id="c8531-223">Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as **kernel applications**.</span></span> <span data-ttu-id="c8531-224">Ogni applicazione di kernel corrisponde a un nodo nel livello di origine indicato come **nodo centrale**.</span><span class="sxs-lookup"><span data-stu-id="c8531-224">Each kernel application corresponds to a node in the source layer, which is referred to as the **central node**.</span></span> <span data-ttu-id="c8531-225">I pesi di un kernel sono condivisi tra molte connessioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-225">The weights of a kernel are shared among many connections.</span></span> <span data-ttu-id="c8531-226">In un'aggregazione convoluzionale ogni kernel è rettangolare e tutte le applicazioni di kernel hanno le stesse dimensioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-226">In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.</span></span>  

<span data-ttu-id="c8531-227">Le aggregazioni convoluzionali supportano gli attributi seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-227">Convolutional bundles support the following attributes:</span></span>

<span data-ttu-id="c8531-228">**InputShape** definisce la dimensionalità del livello di origine ai fini di questa aggregazione convoluzionale.</span><span class="sxs-lookup"><span data-stu-id="c8531-228">**InputShape** defines the dimensionality of the source layer for the purposes of this convolutional bundle.</span></span> <span data-ttu-id="c8531-229">Il valore deve essere una tupla di valori Integer positivi.</span><span class="sxs-lookup"><span data-stu-id="c8531-229">The value must be a tuple of positive integers.</span></span> <span data-ttu-id="c8531-230">Il prodotto dei valori Integer deve essere uguale al numero di nodi del livello di origine, ma non deve corrispondere in altro modo alla dimensionalità dichiarata per il livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-230">The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.</span></span> <span data-ttu-id="c8531-231">La lunghezza di questa tupla diventa il valore di **arietà** per l'aggregazione convoluzionale.</span><span class="sxs-lookup"><span data-stu-id="c8531-231">The length of this tuple becomes the **arity** value for the convolutional bundle.</span></span> <span data-ttu-id="c8531-232">In genere, il grado fa riferimento al numero di argomenti oppure operandi che una funziona può accettare.</span><span class="sxs-lookup"><span data-stu-id="c8531-232">(Typically arity refers to the number of arguments or operands that a function can take.)</span></span>  

<span data-ttu-id="c8531-233">Per definire la forma e le posizioni dei kernel, usare gli attributi **KernelShape**, **Stride**, **Padding**, **LowerPad** e **UpperPad**:</span><span class="sxs-lookup"><span data-stu-id="c8531-233">To define the shape and locations of the kernels, use the attributes **KernelShape**, **Stride**, **Padding**, **LowerPad**, and **UpperPad**:</span></span>   

* <span data-ttu-id="c8531-234">**KernelShape**: (obbligatorio) definisce la dimensionalità di ogni kernel per l'aggregazione convoluzionale.</span><span class="sxs-lookup"><span data-stu-id="c8531-234">**KernelShape**: (required) Defines the dimensionality of each kernel for the convolutional bundle.</span></span> <span data-ttu-id="c8531-235">Il valore deve essere una tupla di numeri interi positivi con una lunghezza uguale al valore del grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-235">The value must be a tuple of positive integers with a length that equals the arity of the bundle.</span></span> <span data-ttu-id="c8531-236">Ogni componente di questa tupla non deve essere maggiore del componente corrispondente di **InputShape**.</span><span class="sxs-lookup"><span data-stu-id="c8531-236">Each component of this tuple must be no greater than the corresponding component of **InputShape**.</span></span> 
* <span data-ttu-id="c8531-237">**Stride**: (facoltativo) definisce le dimensioni del passo di inserimento della convoluzione (una sola dimensione del passo per ogni dimensione), ovvero la distanza tra i nodi centrali.</span><span class="sxs-lookup"><span data-stu-id="c8531-237">**Stride**: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.</span></span> <span data-ttu-id="c8531-238">Il valore deve essere una tupla di numeri interi positivi con una lunghezza corrispondente al valore del grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-238">The value must be a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="c8531-239">Ogni componente di questa tupla non deve essere maggiore del componente corrispondente di **KernelShape**.</span><span class="sxs-lookup"><span data-stu-id="c8531-239">Each component of this tuple must be no greater than the corresponding component of **KernelShape**.</span></span> <span data-ttu-id="c8531-240">Il valore predefinito è una tupla con tutti i componenti uguali a uno.</span><span class="sxs-lookup"><span data-stu-id="c8531-240">The default value is a tuple with all components equal to one.</span></span> 
* <span data-ttu-id="c8531-241">**Sharing**: (facoltativo) definisce la condivisione dei pesi per ogni dimensione della convoluzione.</span><span class="sxs-lookup"><span data-stu-id="c8531-241">**Sharing**: (optional) Defines the weight sharing for each dimension of the convolution.</span></span> <span data-ttu-id="c8531-242">Il valore può essere un singolo valore booleano o una tupla di valori booleani, con lunghezza corrispondente al valore del grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-242">The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="c8531-243">Un singolo valore booleano viene esteso in modo da diventare una tupla di lunghezza corretta con tutti i componenti uguali al valore specificato.</span><span class="sxs-lookup"><span data-stu-id="c8531-243">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="c8531-244">Il valore predefinito è una tupla costituita interamente da valori True.</span><span class="sxs-lookup"><span data-stu-id="c8531-244">The default value is a tuple that consists of all True values.</span></span> 
* <span data-ttu-id="c8531-245">**MapCount**: (facoltativo) definisce il numero di mapping di funzionalità per l'aggregazione convoluzionale.</span><span class="sxs-lookup"><span data-stu-id="c8531-245">**MapCount**: (optional) Defines the number of feature maps for the convolutional bundle.</span></span> <span data-ttu-id="c8531-246">Il valore può essere un singolo numero intero positivo o una tupla di numeri interi positivi, con lunghezza corrispondente al valore del grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-246">The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="c8531-247">Un singolo valore Integer viene esteso in modo da diventare una tupla di lunghezza corretta quando i primi componenti sono uguali al valore specificato e tutti i componenti rimanenti sono uguali a uno.</span><span class="sxs-lookup"><span data-stu-id="c8531-247">A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.</span></span> <span data-ttu-id="c8531-248">Il valore predefinito è uno.</span><span class="sxs-lookup"><span data-stu-id="c8531-248">The default value is one.</span></span> <span data-ttu-id="c8531-249">Il numero totale di mapping di funzionalità è il prodotto dei componenti della tupla.</span><span class="sxs-lookup"><span data-stu-id="c8531-249">The total number of feature maps is the product of the components of the tuple.</span></span> <span data-ttu-id="c8531-250">La fattorizzazione di questo numero totale nei componenti determina il modo in cui i valori di mapping di funzionalità vengono raggruppati nei nodi di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-250">The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.</span></span> 
* <span data-ttu-id="c8531-251">**Weights**: (facoltativo) definisce i pesi iniziali per l'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-251">**Weights**: (optional) Defines the initial weights for the bundle.</span></span> <span data-ttu-id="c8531-252">Il valore deve essere una tupla di valori a virgola mobile con una lunghezza corrispondente al numero di pesi per kernel, come definito in seguito in questo articolo.</span><span class="sxs-lookup"><span data-stu-id="c8531-252">The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.</span></span> <span data-ttu-id="c8531-253">I pesi predefiniti sono generati in modo casuale.</span><span class="sxs-lookup"><span data-stu-id="c8531-253">The default weights are randomly generated.</span></span>  

<span data-ttu-id="c8531-254">Sono disponibili due set di proprietà che controllano la spaziatura interna. Le proprietà si escludono a vicenda:</span><span class="sxs-lookup"><span data-stu-id="c8531-254">There are two sets of properties that control padding, the properties being mutually exclusive:</span></span>

* <span data-ttu-id="c8531-255">**Padding**: (facoltativo) determina se l'input deve essere riempito tramite uno **schema di riempimento predefinito**.</span><span class="sxs-lookup"><span data-stu-id="c8531-255">**Padding**: (optional) Determines whether the input should be padded by using a **default padding scheme**.</span></span> <span data-ttu-id="c8531-256">Il valore può essere un singolo valore booleano o una tupla di valori booleani, con lunghezza corrispondente al valore del grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-256">The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="c8531-257">Un singolo valore booleano viene esteso in modo da diventare una tupla di lunghezza corretta con tutti i componenti uguali al valore specificato.</span><span class="sxs-lookup"><span data-stu-id="c8531-257">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="c8531-258">Se il valore per una dimensione è True, l'origine sarà riempita in modo logico in quella dimensione con celle a valore zero per supportare altre applicazioni di kernel, in modo che i nodi centrali del primo e dell'ultimo kernel in quella dimensione siano i primi e gli ultimi nodi di quella dimensione nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-258">If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.</span></span> <span data-ttu-id="c8531-259">Il numero di nodi "fittizi" in ogni dimensione viene quindi determinato automaticamente, in modo da inserire esattamente i kernel *(InputShape[d] - 1) / Stride[d] + 1* nel livello di origine riempito.</span><span class="sxs-lookup"><span data-stu-id="c8531-259">Thus, the number of "dummy" nodes in each dimension is determined automatically, to fit exactly *(InputShape[d] - 1) / Stride[d] + 1* kernels into the padded source layer.</span></span> <span data-ttu-id="c8531-260">Se il valore per una dimensione è False, i kernel verranno definiti in modo che il numero di nodi esclusi in ogni lato sia uguale (con una differenza massima di 1).</span><span class="sxs-lookup"><span data-stu-id="c8531-260">If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).</span></span> <span data-ttu-id="c8531-261">Il valore predefinito di questo attributo è una tupla con tutti i componenti uguali a False.</span><span class="sxs-lookup"><span data-stu-id="c8531-261">The default value of this attribute is a tuple with all components equal to False.</span></span>
* <span data-ttu-id="c8531-262">**UpperPad** e **LowerPad**: (facoltativi) consentono un maggiore controllo sulla quantità di riempimento da usare.</span><span class="sxs-lookup"><span data-stu-id="c8531-262">**UpperPad** and **LowerPad**: (optional) Provide greater control over the amount of padding to use.</span></span> <span data-ttu-id="c8531-263">**Importante:** questi attributi possono essere definiti solo se la proprietà **Padding** precedente ***non*** è definita.</span><span class="sxs-lookup"><span data-stu-id="c8531-263">**Important:** These attributes can be defined if and only if the **Padding** property above is ***not*** defined.</span></span> <span data-ttu-id="c8531-264">I valori devono essere tuple con numeri interi con lunghezza corrispondente al grado dell'aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-264">The values should be integer-valued tuples with lengths that are the arity of the bundle.</span></span> <span data-ttu-id="c8531-265">Quando questi attributi sono specificati, i nodi "fittizi" vengono aggiunti alle estremità superiori e inferiori di ogni dimensione del livello di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-265">When these attributes are specified, "dummy" nodes are added to the lower and upper ends of each dimension of the input layer.</span></span> <span data-ttu-id="c8531-266">Il numero di nodi aggiunti alle estremità inferiori e superiori di ogni dimensione è determinato rispettivamente da **LowerPad**[i] e **UpperPad**[i].</span><span class="sxs-lookup"><span data-stu-id="c8531-266">The number of nodes added to the lower and upper ends in each dimension is determined by **LowerPad**[i] and **UpperPad**[i] respectively.</span></span> <span data-ttu-id="c8531-267">Per assicurare che i kernel corrispondano solo a nodi "effettivi" e non a nodi "fittizi", è necessario che siano soddisfatte le condizioni seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-267">To ensure that kernels correspond only to "real" nodes and not to "dummy" nodes, the following conditions must be met:</span></span>
  * <span data-ttu-id="c8531-268">Ogni componente di**LowerPad** deve essere rigorosamente minore di KernelShape[d]/2.</span><span class="sxs-lookup"><span data-stu-id="c8531-268">Each component of **LowerPad** must be strictly less than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="c8531-269">Ogni componente di **UpperPad** non deve essere maggiore di KernelShape[d]/2.</span><span class="sxs-lookup"><span data-stu-id="c8531-269">Each component of **UpperPad** must be no greater than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="c8531-270">Il valore predefinito di questi attributi è una tupla con tutti i componenti uguali a 0.</span><span class="sxs-lookup"><span data-stu-id="c8531-270">The default value of these attributes is a tuple with all components equal to 0.</span></span> 

<span data-ttu-id="c8531-271">L'impostazione **Padding** = true consente tutto il riempimento necessario per mantenere il "centro" del kernel all'interno dell'input "reale".</span><span class="sxs-lookup"><span data-stu-id="c8531-271">The setting **Padding** = true allows as much padding as is needed to keep the "center" of the kernel inside the "real" input.</span></span> <span data-ttu-id="c8531-272">In questo modo i calcoli matematici variano un po' per calcolare le dimensioni di output.</span><span class="sxs-lookup"><span data-stu-id="c8531-272">This changes the math a bit for computing the output size.</span></span> <span data-ttu-id="c8531-273">In genere, le dimensioni di output *D* vengono calcolate come *D = (I - K) / S + 1*, dove *I* è la dimensione di input, *K* è la dimensione del kernel, *S* è lo stride e */* è la divisione intera (con arrotondamento a zero).</span><span class="sxs-lookup"><span data-stu-id="c8531-273">Generally, the output size *D* is computed as *D = (I - K) / S + 1*, where *I* is the input size, *K* is the kernel size, *S* is the stride, and */* is integer division (round toward zero).</span></span> <span data-ttu-id="c8531-274">Se si imposta UpperPad = [1, 1], la dimensione di input *I* è in realtà 29, ovvero *D = (29 - 5) / 2 + 1 = 13*.</span><span class="sxs-lookup"><span data-stu-id="c8531-274">If you set UpperPad = [1, 1], the input size *I* is effectively 29, and thus *D = (29 - 5) / 2 + 1 = 13*.</span></span> <span data-ttu-id="c8531-275">Se **Padding** è true, *I* viene essenzialmente incrementato di *K - 1*, ovvero *D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14*.</span><span class="sxs-lookup"><span data-stu-id="c8531-275">However, when **Padding** = true, essentially *I* gets bumped up by *K - 1*; hence *D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14*.</span></span> <span data-ttu-id="c8531-276">Specificando i valori per **UpperPad** e **LowerPad** si ottiene un maggiore controllo sul riempimento rispetto all'impostazione **Padding** = true.</span><span class="sxs-lookup"><span data-stu-id="c8531-276">By specifying values for **UpperPad** and **LowerPad** you get much more control over the padding than if you just set **Padding** = true.</span></span>

<span data-ttu-id="c8531-277">Per altre informazioni sulle reti convoluzionali e le relative applicazioni, vedere gli articoli seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-277">For more information about convolutional networks and their applications, see these articles:</span></span>  

* [<span data-ttu-id="c8531-278">http://deeplearning.net/tutorial/lenet.html </span><span class="sxs-lookup"><span data-stu-id="c8531-278">http://deeplearning.net/tutorial/lenet.html </span></span>](http://deeplearning.net/tutorial/lenet.html)
* [<span data-ttu-id="c8531-279">http://research.microsoft.com/pubs/68920/icdar03.pdf</span><span class="sxs-lookup"><span data-stu-id="c8531-279">http://research.microsoft.com/pubs/68920/icdar03.pdf</span></span>](http://research.microsoft.com/pubs/68920/icdar03.pdf) 
* [<span data-ttu-id="c8531-280">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</span><span class="sxs-lookup"><span data-stu-id="c8531-280">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</span></span>](http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf)  

## <a name="pooling-bundles"></a><span data-ttu-id="c8531-281">Aggregazioni di pooling</span><span class="sxs-lookup"><span data-stu-id="c8531-281">Pooling bundles</span></span>
<span data-ttu-id="c8531-282">Un'**aggregazione di pooling** applica una geometria analoga alla connettività convoluzionale, ma usa funzioni predefinite per i valori del nodo di origine per derivare il valore del nodo di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-282">A **pooling bundle** applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.</span></span> <span data-ttu-id="c8531-283">Le aggregazioni di pooling non hanno quindi stati sottoponibili a training (pesi o distorsioni).</span><span class="sxs-lookup"><span data-stu-id="c8531-283">Hence, pooling bundles have no trainable state (weights or biases).</span></span> <span data-ttu-id="c8531-284">Le aggregazioni di pooling supportano tutti gli attributi convoluzionali, ad eccezione di **Sharing**, **MapCount** e **Weights**.</span><span class="sxs-lookup"><span data-stu-id="c8531-284">Pooling bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

<span data-ttu-id="c8531-285">In genere, i kernel riepilogati da unità di pooling adiacenti non sono sovrapposti.</span><span class="sxs-lookup"><span data-stu-id="c8531-285">Typically, the kernels summarized by adjacent pooling units do not overlap.</span></span> <span data-ttu-id="c8531-286">Se Stride[d] equivale a KernelShape[d] in ogni dimensione, il livello ottenuto è il livello di pooling locale tradizionale, usato in genere nelle reti neurali convoluzionali.</span><span class="sxs-lookup"><span data-stu-id="c8531-286">If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.</span></span> <span data-ttu-id="c8531-287">Ogni nodo di destinazione calcola il valore massimo o medio delle attività del rispettivo kernel nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-287">Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.</span></span>  

<span data-ttu-id="c8531-288">L'esempio seguente illustra un'aggregazione di pooling:</span><span class="sxs-lookup"><span data-stu-id="c8531-288">The following example illustrates a pooling bundle:</span></span> 

    hidden P1 [5, 12, 12]
      from C1 max pool {
        InputShape  = [ 5, 24, 24];
        KernelShape = [ 1,  2,  2];
        Stride      = [ 1,  2,  2];
      }  

* <span data-ttu-id="c8531-289">L'arietà dell'aggregazione è pari a 3, ovvero alla lunghezza delle tuple **InputShape**, **KernelShape** e **Stride**.</span><span class="sxs-lookup"><span data-stu-id="c8531-289">The arity of the bundle is 3 (the length of the tuples **InputShape**, **KernelShape**, and **Stride**).</span></span> 
* <span data-ttu-id="c8531-290">Il numero di nodi del livello di origine è pari a *5 * 24 * 24 = 2880*.</span><span class="sxs-lookup"><span data-stu-id="c8531-290">The number of nodes in the source layer is *5 * 24 * 24 = 2880*.</span></span> 
* <span data-ttu-id="c8531-291">Questo è un livello di pooling locale tradizionale, perché **KernelShape** e **Stride** sono uguali.</span><span class="sxs-lookup"><span data-stu-id="c8531-291">This is a traditional local pooling layer because **KernelShape** and **Stride** are equal.</span></span> 
* <span data-ttu-id="c8531-292">Il numero di nodi del livello di destinazione è pari a *5 * 12 * 12 = 1440*.</span><span class="sxs-lookup"><span data-stu-id="c8531-292">The number of nodes in the destination layer is *5 * 12 * 12 = 1440*.</span></span>  

<span data-ttu-id="c8531-293">Per altre informazioni sui livelli di pooling, vedere gli articoli seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-293">For more information about pooling layers, see these articles:</span></span>  

* <span data-ttu-id="c8531-294">[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Sezione 3.4)</span><span class="sxs-lookup"><span data-stu-id="c8531-294">[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Section 3.4)</span></span>
* [<span data-ttu-id="c8531-295">http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf</span><span class="sxs-lookup"><span data-stu-id="c8531-295">http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf</span></span>](http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf) 
* [<span data-ttu-id="c8531-296">http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf</span><span class="sxs-lookup"><span data-stu-id="c8531-296">http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf</span></span>](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf)

## <a name="response-normalization-bundles"></a><span data-ttu-id="c8531-297">Aggregazioni di normalizzazione delle risposte</span><span class="sxs-lookup"><span data-stu-id="c8531-297">Response normalization bundles</span></span>
<span data-ttu-id="c8531-298">La **normalizzazione delle risposte** è uno schema di normalizzazione locale introdotto per la prima volta da Geoffrey Hinton e altri esperti nell'articolo [ImageNet Classiﬁcation with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Classificazione ImageNet con reti neurali convoluzionali avanzate).</span><span class="sxs-lookup"><span data-stu-id="c8531-298">**Response normalization** is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in the paper [ImageNet Classiﬁcation with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf).</span></span> <span data-ttu-id="c8531-299">La normalizzazione delle risposte viene usata per semplificare la generalizzazione nelle reti neurali.</span><span class="sxs-lookup"><span data-stu-id="c8531-299">Response normalization is used to aid generalization in neural nets.</span></span> <span data-ttu-id="c8531-300">Quando un neurone opera a un livello di attivazione molto elevato, un livello di normalizzazione delle risposte locale sopprime il livello di attivazione dei neuroni circostanti.</span><span class="sxs-lookup"><span data-stu-id="c8531-300">When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.</span></span> <span data-ttu-id="c8531-301">Ciò avviene tramite tre parametri (***α***, ***β***, e ***k***) e una struttura convoluzionale (o forma di vicinato).</span><span class="sxs-lookup"><span data-stu-id="c8531-301">This is done by using three parameters (***α***, ***β***, and ***k***) and a convolutional structure (or neighborhood shape).</span></span> <span data-ttu-id="c8531-302">Ogni neurone nel livello di destinazione ***y*** corrisponde a un neurone ***x*** nel livello di origine.</span><span class="sxs-lookup"><span data-stu-id="c8531-302">Every neuron in the destination layer ***y*** corresponds to a neuron ***x*** in the source layer.</span></span> <span data-ttu-id="c8531-303">Il livello di attivazione di ***y*** è dato dalla formula seguente, dove ***f*** corrisponde al livello di attivazione di un neurone e ***Nx*** è il kernel o l'insieme contenente i neuroni nel vicinato di ***x***, come definito dalla struttura convoluzionale seguente:</span><span class="sxs-lookup"><span data-stu-id="c8531-303">The activation level of ***y*** is given by the following formula, where ***f*** is the activation level of a neuron, and ***Nx*** is the kernel (or the set that contains the neurons in the neighborhood of ***x***), as defined by the following convolutional structure:</span></span>  

![][1]  

<span data-ttu-id="c8531-304">Le aggregazioni di normalizzazione delle risposte supportano tutti gli attributi convoluzionali, ad eccezione di **Sharing**, **MapCount** e **Weights**.</span><span class="sxs-lookup"><span data-stu-id="c8531-304">Response normalization bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

* <span data-ttu-id="c8531-305">Se il kernel contiene neuroni nello stesso mapping di ***x***, lo schema di normalizzazione è definito **normalizzazione dello stesso mapping**.</span><span class="sxs-lookup"><span data-stu-id="c8531-305">If the kernel contains neurons in the same map as ***x***, the normalization scheme is referred to as **same map normalization**.</span></span> <span data-ttu-id="c8531-306">Per definire la normalizzazione nello stesso mapping, la prima coordinata in **InputShape** deve avere valore 1.</span><span class="sxs-lookup"><span data-stu-id="c8531-306">To define same map normalization, the first coordinate in **InputShape** must have the value 1.</span></span>
* <span data-ttu-id="c8531-307">Se il kernel contiene neuroni nella stessa posizione spaziale di ***x***, ma i neuroni si trovano in mapping diversi, lo schema di normalizzazione sarà definito **normalizzazione tra mapping**.</span><span class="sxs-lookup"><span data-stu-id="c8531-307">If the kernel contains neurons in the same spatial position as ***x***, but the neurons are in other maps, the normalization scheme is called **across maps normalization**.</span></span> <span data-ttu-id="c8531-308">Questo tipo di normalizzazione delle risposte implementa una forma di inibizione laterale ispirata dal tipo trovato nei neuroni reali, creando una competizione per livelli di attivazione elevati tra gli output di neuroni calcolati nei diversi mapping.</span><span class="sxs-lookup"><span data-stu-id="c8531-308">This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.</span></span> <span data-ttu-id="c8531-309">Per definire la normalizzazione tra mapping, è necessario che la prima coordinata sia superiore a uno e non sia superiore al numero di mapping e che il resto delle coordinate abbia valore 1.</span><span class="sxs-lookup"><span data-stu-id="c8531-309">To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.</span></span>  

<span data-ttu-id="c8531-310">Poiché le aggregazioni di normalizzazione delle risposte applicano una funzione predefinita ai valori del nodo di origine per determinare il valore del nodo di destinazione, non hanno stati sottoponibili a training (pesi o distorsioni).</span><span class="sxs-lookup"><span data-stu-id="c8531-310">Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).</span></span>   

<span data-ttu-id="c8531-311">**Avviso**: i nodi del livello di destinazione corrispondono ai neuroni che costituiscono i nodi centrali dei kernel.</span><span class="sxs-lookup"><span data-stu-id="c8531-311">**Alert**: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.</span></span> <span data-ttu-id="c8531-312">Ad esempio, se KernelShape[d] è dispari, *KernelShape[d]/2* corrisponderà al nodo centrale del kernel.</span><span class="sxs-lookup"><span data-stu-id="c8531-312">For example, if KernelShape[d] is odd, then *KernelShape[d]/2* corresponds to the central kernel node.</span></span> <span data-ttu-id="c8531-313">Se *KernelShape[d]* è pari, il nodo centrale si trova in *KernelShape[d]/2 - 1*.</span><span class="sxs-lookup"><span data-stu-id="c8531-313">If *KernelShape[d]* is even, the central node is at *KernelShape[d]/2 - 1*.</span></span> <span data-ttu-id="c8531-314">Se quindi **Padding**[d] è False, il primo e l'ultimo nodo *KernelShape[d]/2* non hanno nodi corrispondenti nel livello di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-314">Therefore, if **Padding**[d] is False, the first and the last *KernelShape[d]/2* nodes do not have corresponding nodes in the destination layer.</span></span> <span data-ttu-id="c8531-315">Per evitare questa situazione, definire **Padding** come [true, true, …, true].</span><span class="sxs-lookup"><span data-stu-id="c8531-315">To avoid this situation, define **Padding** as [true, true, …, true].</span></span>  

<span data-ttu-id="c8531-316">Oltre ai quattro attributi descritti precedentemente, le aggregazioni di normalizzazione delle risposte supportano anche gli attributi seguenti:</span><span class="sxs-lookup"><span data-stu-id="c8531-316">In addition to the four attributes described earlier, response normalization bundles also support the following attributes:</span></span>  

* <span data-ttu-id="c8531-317">**Alpha**: (obbligatorio) specifica un valore a virgola mobile che corrisponde a ***α*** nella formula precedente.</span><span class="sxs-lookup"><span data-stu-id="c8531-317">**Alpha**: (required) Specifies a floating-point value that corresponds to ***α*** in the previous formula.</span></span> 
* <span data-ttu-id="c8531-318">**Beta**: (obbligatorio) specifica un valore a virgola mobile che corrisponde a ***β*** nella formula precedente.</span><span class="sxs-lookup"><span data-stu-id="c8531-318">**Beta**: (required) Specifies a floating-point value that corresponds to ***β*** in the previous formula.</span></span> 
* <span data-ttu-id="c8531-319">**Offset**: (facoltativo) specifica un valore a virgola mobile che corrisponde a ***k*** nella formula precedente.</span><span class="sxs-lookup"><span data-stu-id="c8531-319">**Offset**: (optional) Specifies a floating-point value that corresponds to ***k*** in the previous formula.</span></span> <span data-ttu-id="c8531-320">Assume il valore predefinito 1.</span><span class="sxs-lookup"><span data-stu-id="c8531-320">It defaults to 1.</span></span>  

<span data-ttu-id="c8531-321">L'esempio seguente definisce un'aggregazione di normalizzazione delle risposte usando questi attributi:</span><span class="sxs-lookup"><span data-stu-id="c8531-321">The following example defines a response normalization bundle using these attributes:</span></span>  

    hidden RN1 [5, 10, 10]
      from P1 response norm {
        InputShape  = [ 5, 12, 12];
        KernelShape = [ 1,  3,  3];
        Alpha = 0.001;
        Beta = 0.75;
      }  

* <span data-ttu-id="c8531-322">Il livello di origine include cinque mapping, ognuno con una dimensione di 12x12, per un totale di 1440 nodi.</span><span class="sxs-lookup"><span data-stu-id="c8531-322">The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.</span></span> 
* <span data-ttu-id="c8531-323">Il valore di **KernelShape** indica che si tratta dello stesso livello di normalizzazione di mapping, in cui il vicinato è un rettangolo 3x3.</span><span class="sxs-lookup"><span data-stu-id="c8531-323">The value of **KernelShape** indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle.</span></span> 
* <span data-ttu-id="c8531-324">Se il valore predefinito di **Padding** è False, il livello di destinazione ha solo 10 nodi in ogni dimensione.</span><span class="sxs-lookup"><span data-stu-id="c8531-324">The default value of **Padding** is False, thus the destination layer has only 10 nodes in each dimension.</span></span> <span data-ttu-id="c8531-325">Per includere un nodo nel livello di destinazione corrispondente a ogni nodo nel livello di origine, aggiungere Padding = [true, true, true]; e modificare le dimensioni di RN1 su [5, 12, 12].</span><span class="sxs-lookup"><span data-stu-id="c8531-325">To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].</span></span>  

## <a name="share-declaration"></a><span data-ttu-id="c8531-326">Dichiarazione delle condivisioni</span><span class="sxs-lookup"><span data-stu-id="c8531-326">Share declaration</span></span>
<span data-ttu-id="c8531-327">Net# supporta facoltativamente la definizione di più aggregazioni con pesi condivisi.</span><span class="sxs-lookup"><span data-stu-id="c8531-327">Net# optionally supports defining multiple bundles with shared weights.</span></span> <span data-ttu-id="c8531-328">I pesi di qualsiasi aggregazione possono essere condivisi se le loro strutture sono uguali.</span><span class="sxs-lookup"><span data-stu-id="c8531-328">The weights of any two bundles can be shared if their structures are the same.</span></span> <span data-ttu-id="c8531-329">La sintassi seguente definisce le aggregazioni con i pesi condivisi:</span><span class="sxs-lookup"><span data-stu-id="c8531-329">The following syntax defines bundles with shared weights:</span></span>  

    share-declaration:
        share    {    layer-list    }
        share    {    bundle-list    }
       share    {    bias-list    }

    layer-list:
        layer-name    ,    layer-name
        layer-list    ,    layer-name

    bundle-list:
       bundle-spec    ,    bundle-spec
        bundle-list    ,    bundle-spec

    bundle-spec:
       layer-name    =>     layer-name

    bias-list:
        bias-spec    ,    bias-spec
        bias-list    ,    bias-spec

    bias-spec:
        1    =>    layer-name

    layer-name:
        identifier  

<span data-ttu-id="c8531-330">Ad esempio, la dichiarazione di condivisioni seguente specifica i nomi dei livelli, indicando che devono essere condivisi sia i pesi che le distorsioni:</span><span class="sxs-lookup"><span data-stu-id="c8531-330">For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:</span></span>  

    Const {
      InputSize = 37;
      HiddenSize = 50;
    }
    input {
      Data1 [InputSize];
      Data2 [InputSize];
    }
    hidden {
      H1 [HiddenSize] from Data1 all;
      H2 [HiddenSize] from Data2 all;
    }
    output Result [2] {
      from H1 all;
      from H2 all;
    }
    share { H1, H2 } // share both weights and biases  

* <span data-ttu-id="c8531-331">Le funzionalità di input sono partizionate in due livelli di input di dimensioni identiche.</span><span class="sxs-lookup"><span data-stu-id="c8531-331">The input features are partitioned into two equal sized input layers.</span></span> 
* <span data-ttu-id="c8531-332">I livelli nascosti calcolano quindi le funzionalità di livello superiore nei due livelli di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-332">The hidden layers then compute higher level features on the two input layers.</span></span> 
* <span data-ttu-id="c8531-333">La dichiarazione di condivisione specifica che *H1* e *H2* devono essere calcolati in modo analogo dai rispettivi input.</span><span class="sxs-lookup"><span data-stu-id="c8531-333">The share-declaration specifies that *H1* and *H2* must be computed in the same way from their respective inputs.</span></span>  

<span data-ttu-id="c8531-334">In alternativa, è possibile specificare questo concetto con due dichiarazioni delle condivisioni separate, come indicato di seguito:</span><span class="sxs-lookup"><span data-stu-id="c8531-334">Alternatively, this could be specified with two separate share-declarations as follows:</span></span>  

    share { Data1 => H1, Data2 => H2 } // share weights  

<!-- -->

    share { 1 => H1, 1 => H2 } // share biases  

<span data-ttu-id="c8531-335">È possibile usare la forma breve solo se i livelli contengono una singola aggregazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-335">You can use the short form only when the layers contain a single bundle.</span></span> <span data-ttu-id="c8531-336">In genere, la condivisione è possibile solo se la struttura rilevante è identica, ovvero se le dimensioni, la geometria convoluzionale e così via sono identiche.</span><span class="sxs-lookup"><span data-stu-id="c8531-336">In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.</span></span>  

## <a name="examples-of-net-usage"></a><span data-ttu-id="c8531-337">Esempi di utilizzo di Net#</span><span class="sxs-lookup"><span data-stu-id="c8531-337">Examples of Net# usage</span></span>
<span data-ttu-id="c8531-338">Questa sezione offre alcuni esempi di utilizzo di Net# per aggiungere livelli nascosti, definire il modo in cui i livelli nascosti interagiscono con altri livelli e creare reti convoluzionali.</span><span class="sxs-lookup"><span data-stu-id="c8531-338">This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.</span></span>   

### <a name="define-a-simple-custom-neural-network-hello-world-example"></a><span data-ttu-id="c8531-339">Definire una semplice rete neurale personalizzata, ad esempio "Hello World"</span><span class="sxs-lookup"><span data-stu-id="c8531-339">Define a simple custom neural network: "Hello World" example</span></span>
<span data-ttu-id="c8531-340">Questo esempio illustra come creare un modello di rete neurale con un singolo livello nascosto.</span><span class="sxs-lookup"><span data-stu-id="c8531-340">This simple example demonstrates how to create a neural network model that has a single hidden layer.</span></span>  

    input Data auto;
    hidden H [200] from Data all;
    output Out [10] sigmoid from H all;  

<span data-ttu-id="c8531-341">L'esempio illustra alcuni comandi di base come indicato di seguito:</span><span class="sxs-lookup"><span data-stu-id="c8531-341">The example illustrates some basic commands as follows:</span></span>  

* <span data-ttu-id="c8531-342">La prima riga definisce il livello di input (denominato *Data*).</span><span class="sxs-lookup"><span data-stu-id="c8531-342">The first line defines the input layer (named *Data*).</span></span> <span data-ttu-id="c8531-343">Quando si usa la parola chiave **auto**, la rete neurale include automaticamente tutte le colonne di funzionalità negli esempi di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-343">When you use the  **auto** keyword, the neural network automatically includes all feature columns in the input examples.</span></span> 
* <span data-ttu-id="c8531-344">La seconda riga crea il livello nascosto.</span><span class="sxs-lookup"><span data-stu-id="c8531-344">The second line creates the hidden layer.</span></span> <span data-ttu-id="c8531-345">Il nome *H* è assegnato al livello nascosto, che ha 200 nodi.</span><span class="sxs-lookup"><span data-stu-id="c8531-345">The name *H* is assigned to the hidden layer, which has 200 nodes.</span></span> <span data-ttu-id="c8531-346">Questo livello è completamente connesso al livello di input.</span><span class="sxs-lookup"><span data-stu-id="c8531-346">This layer is fully connected to the input layer.</span></span>
* <span data-ttu-id="c8531-347">La terza riga definisce il livello di output (denominato *O*), che contiene 10 nodi di output.</span><span class="sxs-lookup"><span data-stu-id="c8531-347">The third line defines the output layer (named *O*), which contains 10 output nodes.</span></span> <span data-ttu-id="c8531-348">Se la rete neurale viene usata per la classificazione, è disponibile un nodo di output per ogni classe.</span><span class="sxs-lookup"><span data-stu-id="c8531-348">If the neural network is used for classification, there is one output node per class.</span></span> <span data-ttu-id="c8531-349">La parola chiave **sigmoid** indica la funzione di output applicata al livello di output.</span><span class="sxs-lookup"><span data-stu-id="c8531-349">The keyword **sigmoid** indicates that the output function is applied to the output layer.</span></span>   

### <a name="define-multiple-hidden-layers-computer-vision-example"></a><span data-ttu-id="c8531-350">Definire più livelli nascosti: esempio obiettivo computer</span><span class="sxs-lookup"><span data-stu-id="c8531-350">Define multiple hidden layers: computer vision example</span></span>
<span data-ttu-id="c8531-351">L'esempio seguente illustra come definire una rete neurale leggermente più complessa, con più livelli nascosti personalizzati.</span><span class="sxs-lookup"><span data-stu-id="c8531-351">The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.</span></span>  

    // Define the input layers 
    input Pixels [10, 20];
    input MetaData [7];

    // Define the first two hidden layers, using data only from the Pixels input
    hidden ByRow [10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol [5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;

    // Define the third hidden layer, which uses as source the hidden layers ByRow and ByCol
    hidden Gather [100] 
    {
      from ByRow all;
      from ByCol all;
    }

    // Define the output layer and its sources
    output Result [10]  
    {
      from Gather all;
      from MetaData all;
    }  

<span data-ttu-id="c8531-352">Questo esempio descrive diverse funzionalità del linguaggio di specifica delle reti neurali:</span><span class="sxs-lookup"><span data-stu-id="c8531-352">This example illustrates several features of the neural networks specification language:</span></span>  

* <span data-ttu-id="c8531-353">La struttura ha due livelli di input, *Pixels* e *MetaData*.</span><span class="sxs-lookup"><span data-stu-id="c8531-353">The structure has two input layers, *Pixels* and *MetaData*.</span></span>
* <span data-ttu-id="c8531-354">Il livello *Pixels* è un livello di origine per due aggregazioni di connessioni, con livelli di destinazione, *ByRow* e *ByCol*.</span><span class="sxs-lookup"><span data-stu-id="c8531-354">The *Pixels* layer is a source layer for two connection bundles, with destination layers, *ByRow* and *ByCol*.</span></span>
* <span data-ttu-id="c8531-355">I livelli *Gather* e *Result* sono livelli di destinazione in più aggregazioni di connessioni.</span><span class="sxs-lookup"><span data-stu-id="c8531-355">The layers *Gather* and *Result* are destination layers in multiple connection bundles.</span></span>
* <span data-ttu-id="c8531-356">Il livello di output, *Result*, è un livello di destinazione in due aggregazioni di connessione. Una con il secondo livello nascosto (Gather) come livello di destinazione e l'altra con il livello di input (MetaData) come livello di destinazione.</span><span class="sxs-lookup"><span data-stu-id="c8531-356">The output layer, *Result*, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.</span></span>
* <span data-ttu-id="c8531-357">I livelli nascosti, *ByRow* e *ByCol*, specificano connettività filtrata usando espressioni del predicato.</span><span class="sxs-lookup"><span data-stu-id="c8531-357">The hidden layers, *ByRow* and *ByCol*, specify filtered connectivity by using predicate expressions.</span></span> <span data-ttu-id="c8531-358">Più precisamente, il nodo in *ByRow* su [x, y] è connesso ai nodi in *Pixels* che hanno la prima coordinata indice uguale alla prima coordinata del nodo x.</span><span class="sxs-lookup"><span data-stu-id="c8531-358">More precisely, the node in *ByRow* at [x, y] is connected to the nodes in *Pixels* that have the first index coordinate equal to the node's first coordinate, x.</span></span> <span data-ttu-id="c8531-359">Analogamente, il nodo in *ByCol in [x, y] è connesso ai nodi in _Pixels* che hanno la seconda coordinata indice all'interno di una della seconda coordinata del nodo y.</span><span class="sxs-lookup"><span data-stu-id="c8531-359">Similarly, the node in *ByCol at [x, y] is connected to the nodes in _Pixels* that have the second index coordinate within one of the node's second coordinate, y.</span></span>  

### <a name="define-a-convolutional-network-for-multiclass-classification-digit-recognition-example"></a><span data-ttu-id="c8531-360">Definire una rete per la classificazione multiclasse convoluzionale: esempio di riconoscimento cifra</span><span class="sxs-lookup"><span data-stu-id="c8531-360">Define a convolutional network for multiclass classification: digit recognition example</span></span>
<span data-ttu-id="c8531-361">La definizione della rete seguente è progettata per riconoscere numeri e illustra alcune tecniche avanzate per la personalizzazione di una rete neurale.</span><span class="sxs-lookup"><span data-stu-id="c8531-361">The definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.</span></span>  

    input Image [29, 29];
    hidden Conv1 [5, 13, 13] from Image convolve 
    {
       InputShape  = [29, 29];
       KernelShape = [ 5,  5];
       Stride      = [ 2,  2];
       MapCount    = 5;
    }
    hidden Conv2 [50, 5, 5]
    from Conv1 convolve 
    {
       InputShape  = [ 5, 13, 13];
       KernelShape = [ 1,  5,  5];
       Stride      = [ 1,  2,  2];
       Sharing     = [false, true, true];
       MapCount    = 10;
    }
    hidden Hid3 [100] from Conv2 all;
    output Digit [10] from Hid3 all;  


* <span data-ttu-id="c8531-362">La struttura ha un livello di input singolo, *Image*.</span><span class="sxs-lookup"><span data-stu-id="c8531-362">The structure has a single input layer, *Image*.</span></span>
* <span data-ttu-id="c8531-363">La parola chiave **convolve** indica che i livelli *Conv1* e *Conv2* sono livelli convoluzionali.</span><span class="sxs-lookup"><span data-stu-id="c8531-363">The keyword **convolve** indicates that the layers named *Conv1* and *Conv2* are convolutional layers.</span></span> <span data-ttu-id="c8531-364">Tutte queste dichiarazioni di livelli sono seguite da un elenco di attributi convoluzionali.</span><span class="sxs-lookup"><span data-stu-id="c8531-364">Each of these layer declarations is followed by a list of the convolution attributes.</span></span>
* <span data-ttu-id="c8531-365">La rete ha un terzo livello nascosto, *Hid3*, che è completamente connesso al secondo livello nascosto, *Conv2*.</span><span class="sxs-lookup"><span data-stu-id="c8531-365">The net has a third hidden layer, *Hid3*, which is fully connected to the second hidden layer, *Conv2*.</span></span>
* <span data-ttu-id="c8531-366">Il livello di output, *Digit*, è connesso solo al terzo livello nascosto, *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="c8531-366">The output layer, *Digit*, is connected only to the third hidden layer, *Hid3*.</span></span> <span data-ttu-id="c8531-367">La parola chiave **all** indica che il livello di output è completamente connesso a *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="c8531-367">The keyword **all** indicates that the output layer is fully connected to *Hid3*.</span></span>
* <span data-ttu-id="c8531-368">L'arietà della convoluzione è uguale a tre, ovvero alla lunghezza delle tuple **InputShape**, **KernelShape**, **Stride** e **Sharing**.</span><span class="sxs-lookup"><span data-stu-id="c8531-368">The arity of the convolution is three (the length of the tuples **InputShape**, **KernelShape**, **Stride**, and **Sharing**).</span></span> 
* <span data-ttu-id="c8531-369">Il numero di pesi per kernel è*1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26. Oppure 26 * 50 = 1300*.</span><span class="sxs-lookup"><span data-stu-id="c8531-369">The number of weights per kernel is *1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26. Or 26 * 50 = 1300*.</span></span>
* <span data-ttu-id="c8531-370">È possibile calcolare i nodi in ogni livello nascosto come indicato di seguito:</span><span class="sxs-lookup"><span data-stu-id="c8531-370">You can calculate the nodes in each hidden layer as follows:</span></span>
  * <span data-ttu-id="c8531-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="c8531-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span></span>
  * <span data-ttu-id="c8531-372">**NodeCount**\[1] = (13 - 5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="c8531-372">**NodeCount**\[1] = (13 - 5) / 2 + 1 = 5.</span></span> 
  * <span data-ttu-id="c8531-373">**NodeCount**\[2] = (13 - 5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="c8531-373">**NodeCount**\[2] = (13 - 5) / 2 + 1 = 5.</span></span> 
* <span data-ttu-id="c8531-374">Il numero totale di nodi può essere calcolato usando la dimensionalità dichiarata del livello, [50, 5, 5], come indicato di seguito: ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span><span class="sxs-lookup"><span data-stu-id="c8531-374">The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span></span>
* <span data-ttu-id="c8531-375">Poiché **Sharing**[d] è False solo per *d == 0*, il numero di kernel è ***MapCount** * **NodeCount**\[0] = 10 * 5 = 50*.</span><span class="sxs-lookup"><span data-stu-id="c8531-375">Because **Sharing**[d] is False only for *d == 0*, the number of kernels is ***MapCount** * **NodeCount**\[0] = 10 * 5 = 50*.</span></span> 

## <a name="acknowledgements"></a><span data-ttu-id="c8531-376">Riconoscimenti</span><span class="sxs-lookup"><span data-stu-id="c8531-376">Acknowledgements</span></span>
<span data-ttu-id="c8531-377">Il linguaggio Net # per personalizzare l'architettura delle reti neurali è stato sviluppato presso Microsoft da Shon Katzenberger (progettista, Machine Learning) e Alexey Kamenev (ingegnere di software, Microsoft Research).</span><span class="sxs-lookup"><span data-stu-id="c8531-377">The Net# language for customizing the architecture of neural networks was developed at Microsoft by Shon Katzenberger (Architect, Machine Learning) and Alexey Kamenev (Software Engineer, Microsoft Research).</span></span> <span data-ttu-id="c8531-378">Viene usato internamente per progetti Machine Learning e le applicazioni che vanno dal rilevamento immagine alle analisi di testo.</span><span class="sxs-lookup"><span data-stu-id="c8531-378">It is used internally for machine learning projects and applications ranging from image detection to text analytics.</span></span> <span data-ttu-id="c8531-379">Per altre informazioni, vedere [Neural Nets in Azure ML - Introduction to Net#](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx) (Reti neurali in Azure ML - Introduzione a Net #)</span><span class="sxs-lookup"><span data-stu-id="c8531-379">For more information, see [Neural Nets in Azure ML - Introduction to Net#](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx)</span></span>

<span data-ttu-id="c8531-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span><span class="sxs-lookup"><span data-stu-id="c8531-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span></span>

